# Dynatrace Documentation: observe/business-observability

Generated: 2026-02-17

Files combined: 18

---


## Source: bo-analysis.md


---
title: Business event analysis and examples
source: https://www.dynatrace.com/docs/observe/business-observability/bo-analysis
scraped: 2026-02-17T21:16:07.718645
---

# Business event analysis and examples

# Business event analysis and examples

* Latest Dynatrace
* Reference
* 9-min read
* Updated on Oct 30, 2023

Once stored in Grail, you can query your business event data interactively and analyze it using [DQL](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language."). DQL is the starting point for analysis, whether you use [Notebooks](/docs/analyze-explore-automate/dashboards-and-notebooks/notebooks "Analyze, visualize, and share insights from your observability dataâall in one collaborative, customizable workspace."), [Dashboards](/docs/analyze-explore-automate/dashboards-and-notebooks/dashboards-new "Create interactive, customizable views to visualize, analyze, and share your observability data in real time."), or the DQL Query API (latest Dynatrace). You can use query results interactively or pin them to a dashboard as charts, tiles, or tables.

## Query and analyze your data

1. Select **Notebooks** ![Notebooks](https://dt-cdn.net/images/notebooks-768-046137830a.webp "Notebooks") or **Dashboards** ![Dashboards](https://dt-cdn.net/images/dashboards-512-b1f1e9690b.png "Dashboards").
2. Build and run your query.

   You can use your queries to create metrics and charts for dashboards. You can also run queries via API to use the data externally.

## Metric examples in Notebooks

Using DQL, you can create metrics on an ongoing basis from any numeric data collected as business events. There are multiple options for charting results in Notebooks ![Notebooks](https://dt-cdn.net/images/notebooks-768-046137830a.webp "Notebooks"), with suggestions that are optimized to your metric data.

### Average trading dollar volume: Single value

This example calculates the average total trading dollar volume generated by EasyTrade in the last 24 hours.

* Assets bought through EasyTrade are captured by the `com.easytrade.quick-buy` and `com.easytrade.long-buy` event types.
* The total trading dollar volume is calculated as the number of assets bought multiplied by the asset price.

The metric below yields a single numerical value, displayed as a **Record list** in Notebooks.

```
fetch bizevents, from:now()-24h, to:now()



| filter event.type == "com.easytrade.quick-buy" or event.type == "com.easytrade.long-buy"



| summarize dollar_volume = avg(amount*price)
```

![Average dollar volume single value](https://dt-cdn.net/images/ba-dollar-volume-single-value-1229-3145c15ba2.webp)

### Total trading dollar volume in time intervals: Line chart

You can create interval-based metrics. This example queries the total trading dollar volume in the last 24 hours in intervals of five minutes. This example uses the DQL [makeTimeseries command](/docs/platform/grail/dynatrace-query-language/commands/aggregation-commands#makeTimeseries "DQL aggregation commands") to build a time series from a measure (the sum of dollar volume), which you can then use to leverage the power of charting in Notebooks or Dashboards.

* Assets sold through EasyTrade are captured by the `com.easytrade.nginx.quick-sell` event type.
* Trading dollar volume is calculated as the number of assets sold multiplied by their price.
* This query returns a value representing the dollar volume in intervals of five minutes.

```
fetch bizevents, from:now()-24h, to:now()



| filter event.type == "com.easytrade.nginx.quick-sell"



| makeTimeseries dollar_volume= sum (amount*price), interval: 5m
```

The image below shows a **Line chart** for query results in Notebooks.

![Dollar volume time intervals line chart](https://dt-cdn.net/images/ba-dollar-volume-time-intervals-line-chart-1316-2a2ac3dd06.webp)

### Average asset price

This example fetches the average asset price for both buy and sell orders through EasyTrade.

* Assets can be bought and sold through EasyTrade, captured, for example, by the `com.easytrade.quick-buy` and `com.easytrade.nginx.quick-sell` event types.
* Without filtering by `event.type`, this query returns the average asset price for both buy and sell orders.

```
fetch bizevents



| filter event.provider == "www.easytrade.com"



| filter isNotNull(price)



| summarize average_price_assets = avg(price)
```

This query is best displayed as a **Single value** (shown below) or a **Record list**.

![Average buy+sell asset price](https://dt-cdn.net/images/ba-avg-asset-price-single-value-1320-26c917a897.webp)

### Daily deposit value in the last 30 days

Find out how much money was transferred to EasyTrade accounts per day, within the last 30 days.

* Money transferred to EasyTrade is captured by the `com.easytrade.deposit` event type.
* The query logic involves:

  + Summarizing the amount of each event to provide the total amount of money transferred to EasyTrade.
  + Splitting this amount into days.

```
fetch bizevents, from: now()-30d, to: now()-1d



| filter event.type == "com.easytrade.deposit"



| makeTimeseries moneyTransfered= sum(amount), interval: 1d
```

The images below show the results as an expanded **Record list** and a **Line chart**.

![Daily deposits record list](https://dt-cdn.net/images/ba-daily-deposits-record-list-1319-eba4e7d2e9.webp)

![Daily deposits line chart](https://dt-cdn.net/images/ba-daily-deposits-line-chart-1315-561171007e.webp)

### Distribution of trades by buckets of 2000 shares

This query shows how many shares were traded in EasyTrade over the last 24 hours.

* This query shows the distribution of trades (or trade counts) by buckets of 2000 affected shares, so we can see how many trades affect 0â2000 shares, 2001â4000 shares, and so on.
* The **Categorical chart** is the best choice for displaying this data in columns showing the number of trades for each bucket.

```
fetch bizevents, from:now()-1d, to:now()



| filter event.provider == "www.easytrade.com"



| filter event.type == "com.easytrade.nginx.long-sell" OR event.type == "com.easytrade.nginx.long-buy"



| summarize numberOfTrades = count(), by:{sharesAffected=bin(amount, 2000)}



| fields sharesAffected = concat(toString(toLong(sharesAffected)+1), " - ", toString(toLong(sharesAffected +2000))), numberOfTrades



| limit 10
```

![Distribution of trades by buckets](https://dt-cdn.net/images/ba-trades-by-buckets-categorical-chart-1316-464499d8c1.webp)

### Time elapsed between events

You need to know how long it takes for a customer to trade, that is, for how much time a customer's money is in the platform before being transferred outside. To achieve this, you need to find out the time elapsed between related events, namely, the first deposit of money in EasyTrade and the first withdrawal of assets.

```
fetch bizevents, from:now()-30d, to:now()



| filter event.provider == "www.easytrade.com"



| sort timestamp, direction:"descending"



| filter event.type == "com.easytrade.deposit" OR event.type == "com.easytrade.withdraw"



| fieldsAdd deposit_ts = if(event.type == "com.easytrade.deposit", timestamp)



| fieldsAdd withdraw_ts = if(event.type == "com.easytrade.withdraw", timestamp)



| summarize {first_deposit_ts = takeFirst(deposit_ts), first_withdraw_ts = takeFirst(withdraw_ts)}, by:{`accountId`}



| fieldsAdd timeDepositToDeposit= (first_withdraw_ts - first_deposit_ts) /(1000000000.0)



| filter timeDepositToDeposit > duration(0,unit:"ns")



| fields `accountId`, `Seconds From Deposit To Deposit`= timeDepositToDeposit
```

The image below shows query results displayed as a **Table** in Notebooks.

![Time between events tabular data](https://dt-cdn.net/images/ba-time-between-events-table-758-6610577a8d.webp)

#### Query steps explained

* **Line 1**

  ```
  fetch bizevents, from:now()-30d, to:now()
  ```

  Fetches the business events table. In addition, the optional `from:` parameter specifies the query start timestamp.
* **Line 2**

  ```
  | filter event.provider == "www.easytrade.com"
  ```

  The [`filter` command](/docs/platform/grail/dynatrace-query-language/commands/filtering-commands#filter "DQL filter and search commands") provides the business event records based on the defined [event provider](/docs/observe/business-observability/bo-events-capturing#configure-sources "Capture business events for Dynatrace Business Observability.").
* **Line 3**

  ```
  | sort timestamp, direction:"descending"
  ```

  To get the most recent events first, you can sort the results by the most recent timestamp.
* **Line 4**

  ```
  | filter event.type == "com.easytrade.deposit" OR event.type == "com.easytrade.withdraw"
  ```

  A new filter combines two different types of events: the deposit of money in EasyTrade and the withdrawal of money from the trading platform.
* **Line 5**

  ```
  | fieldsAdd deposit_ts = if(event.type == "com.easytrade.deposit", timestamp)
  ```

  The [`fieldsAdd` command](/docs/platform/grail/dynatrace-query-language/commands#fields-add "A list of DQL commands.") creates a new field: a timestamp. This is the deposit timestamp.
* **Line 6**

  ```
  | fieldsAdd withdraw_ts = if(event.type == "com.easytrade.withdraw", timestamp)
  ```

  The [`fieldsAdd` command](/docs/platform/grail/dynatrace-query-language/commands#fields-add "A list of DQL commands.") creates a new field: a timestamp. This is the withdrawal timestamp.
* **Line 7**

  ```
  | summarize {first_deposit_ts = takeFirst(deposit_ts), first_withdraw_ts = takeFirst(withdraw_ts)}, by:{`accountId`}
  ```

  The [`summarize` command](/docs/platform/grail/dynatrace-query-language/commands/aggregation-commands#summarize "DQL aggregation commands") groups by account:

  + The first timestamp for a deposit event.
  + The first timestamp for a withdrawal event.
* **Line 8**

  ```
  | fieldsAdd timeDepositToDeposit= (first_withdraw_ts - first_deposit_ts) /(1000000000.0)
  ```

  Creates a new field to calculate the difference between two timestamps in order to calculate the time elapsed between the first deposit and first withdrawal actions per account (in seconds).
* **Line 9**

  ```
  | filter timeDepositToDeposit > duration(0,unit:"ns")
  ```

  This filter command removes all 0 and negative values, so any withdrawal of money that took place before the deposit of money is removed from the results.
* **Line 10**

  ```
  | fields `accountId`, `Seconds From Deposit To Deposit`= timeDepositToDeposit
  ```

  The [`fields` command](/docs/platform/grail/dynatrace-query-language/commands/selection-and-modification-commands#fields "DQL selection and modification commands") restricts the output to the account ID and the time elapsed between the first deposit and the first withdrawal.

### Business events during office hours

To get business events (credit card transactions in this example) occurring only during business hours, choose the business hours to monitor and then remove the non-working days (for example, Saturdays and Sundays) and non-working hours (for example, 5 PM to 6 AM). crediut card transactions

Be conscious of the time zone: if you're based in the UK and analyze events during office hours in a Singapore-based company, timestamps need to be adjusted.

```
fetch bizevents



| filter event.provider == "www.easytrade.com"



| filter isNotNull(`cardType`)



| fieldsAdd hour = getHour(timestamp), day_of_week = getDayOfWeek(timestamp)



| filterOut day_of_week == "Sat" or day_of_week == "Sun" // Remove weekend days



| filterOut hour <= 6 or hour >= 17 // Remove hours not between 6 AM and 5 PM



| fields `Account ID`, event.type, amount, cardType, event.kind
```

The image below shows query results displayed as a **Record list**.

![Events during office hours record list](https://dt-cdn.net/images/ba-events-during-office-hours-record-list-1300-850c8b417e.webp)

### The top five accounts with the greatest decrease in account value

To find the top five accounts with the greatest decrease in the account value in the last 24 hours, this query compares total deposits to total withdrawals.

```
fetch bizevents, from:now()-1d, to:now()



| filter event.provider == "www.easytrade.com"



| filter event.type == "com.easytrade.withdraw" or event.type == "com.easytrade.deposit"



| fieldsAdd amount_withdrawal = if(event.type=="com.easytrade.withdraw", amount, else:0)



| fieldsAdd amount_deposit = if(event.type=="com.easytrade.deposit", amount, else:0)



| summarize {total_withdrawals = sum(amount_withdrawal), total_deposits = sum(amount_deposit)}, by:{accountId}



| fieldsAdd balance_change = total_deposits - total_withdrawals



| sort balance_change, direction:"ascending"



| filter balance_change < 0



| fields accountId, balance_change



| sort balance_change, direction:"ascending"



| limit 5
```

The image below shows query results displayed as a **Record list**.

![Decrease in account value record list](https://dt-cdn.net/images/ba-decrease-in-account-value-record-list-1296-e854ca9c29.webp)

### Missing transactions

A retail company commits to same-day shipping, accepting orders from 8:00 AM to 5:00 PM. They track orders placed, payments confirmed, and orders closed. Shipping is complete by 9:00 PM. each day, at which point the number of events recorded for each event type should be the same; any discrepancies are flagged as an anomaly to be investigated. They run this query at 9:00 PM each day.

* Important event types for this query are `com.acme.order_confirmed`, `com.acme.payment_confirmed`, and `com.acme.close_order`.
* The query generates a series of counts from the three event types.
* The query creates a new field with a condition that should normally be `fulfilled`. Specifically, the condition is fulfilled when the counts for all three event types are identical.

```
`fetch bizevents, from:now()-24h, to:now()



| filter event.provider == "www.acme.com"



| summarize {A_place_order = countIf(event.type=="com.acme.order_confirmed"), B_payment_confirmed = countIf(event.type=="com.acme.payment_confirmed"), C_order_confirmed = countIf(event.type=="com.acme.close_order")},by:{order_id}



| fieldsAdd fulfilled = (A_place_order == B_payment_confirmed and A_place_order == C_order_confirmed)



| filter A_place_order==1
```

Results displayed as a **Table** show which order IDs are not fulfilled (`false`) on the dame day.

![Missing transactions table](https://dt-cdn.net/images/ba-missing-transactions-table-1301-f64b4fbd01.webp)

## Dashboards

You can also use Dashboards ![Dashboards](https://dt-cdn.net/images/dashboards-512-b1f1e9690b.png "Dashboards") to easily implement all the [metric examples in Notebooks](#notebooks) described above. The image below shows a dashboard containing tiles for some of the DQL queries explained above and some other queries that are simple modifications or variations of the examples above.

Select any tile and then select [**Open with** ![Open with](https://dt-cdn.net/images/open-with-003fc82dcd.svg "Open with")](/docs/analyze-explore-automate/dashboards-and-notebooks/dashboards-new#dashboards-open-with "Create interactive, customizable views to visualize, analyze, and share your observability data in real time.") to reuse content from Dashboards in another app, such as Notebooks.

![Business analytics dashboard for metrics](https://dt-cdn.net/images/ba-dashboard-examples-2393-86077d7d44.webp)


---


## Source: bo-api-ingest.md


---
title: Ingest business events via API
source: https://www.dynatrace.com/docs/observe/business-observability/bo-api-ingest
scraped: 2026-02-17T21:21:49.186054
---

# Ingest business events via API

# Ingest business events via API

* Latest Dynatrace
* Reference
* 11-min read
* Updated on Aug 20, 2025

You can configure external business or IT systems to send business events to Dynatrace via API. Additionally, you can query data using DQL via API.

* Business Observability offers the **Business events API** to ingest JSON-format data into Dynatrace via the [`/bizevents/ingest` endpoint](#ingest-endpoint).
* The **Grail - DQL Query API** enables you to access records stored in Grail by making [DQL queries via API](#dql-via-api).

## Business events API: `/bizevents/ingest` endpoint

Business Observability offers the Business events API to ingest JSON data into Dynatrace via the POST method of the `/bizevents/ingest` endpoint.

To try out this API in the API Explorer

1. From the [user menu](/docs/discover-dynatrace/get-started/dynatrace-ui#user-menu-previous-dynatrace "Navigate the latest Dynatrace") (previous Dynatrace), select **Environment API v2**.
2. Expand **Business events**, then **/bizevents/ingest**.

* **Endpoint URL**â`https://{your-environment-id}.live.dynatrace.com/api/v2/bizevents/ingest`
* **Method**âPOST
* **Authentication**â[Access token](#access-token) or [OAuth](#oauth)
* **`Content-Type`** headerâvaries based on the [payload format](#payload-formats).

  + [Pure JSON](#pure-json): `application/json`
  + [CloudEvents](#cloudevents): `application/cloudevent+json` and `application/cloudevents+json`
  + [CloudEvents batch](#cloudevents-batch): `application/cloudevents-batch+json` and `application/cloudevents-batch+json`
* **Request body**â`{your event}`âsee the examples below.

Note that when ingesting business events, Dynatrace stores ingested top-level fields as top-level fields in Grail but converts complex JSON objects into stringsâsee the `customer` field in the [example Grail event](#grail-event) below.

If you are working in the latest Dynatrace, you can access this endpoint via `https://{your-environment-id}.apps.dynatrace.com/platform/classic/environment-api/v2/bizevents/ingest`. This URL only supports [OAuth](#oauth) authentication.

### Supported request formats and examples

In addition to [pure JSON](#pure-json), Dynatrace offers the [CloudEvents](#cloudevents) and [CloudEvents batch](#cloudevents-batch) payload formats, specified in the `Content-Type` request header.

All three formats support the same request payloads. The Business events API limits payload size to 5 MB per request.

#### Pure JSON

There are no mandatory fields.

Example payload in pure JSON for ingesting a single event

```
{



"id":"2",



"paymentType":"paypal",



"plannedDeliveryDate":"01.01.2024",



"event.type":"com.bizevent.single",



"event.provider":"custom.provider",



"total":234,



"customer":{



"firstName":"John",



"lastName":"Doe"



},



"orderItemsProductIDs":[



"PR-102002002",



"QZ-123232"



]



}
```

How Grail stores the single event

Every top-level attribute is stored as a top-level field in Grail; nested JSON objects are stored as strings.

```
{



"timestamp":"2023-08-10T10:06:21.697000000+02:00",



"customer":"{\"firstName\":\"John\",\"lastName\":\"Doe\"}",



"event.id":"6f7c0c12-8386-4015-bcb2-296ba73ebd54",



"event.kind":"BIZ_EVENT",



"event.provider":"custom.provider",



"event.type":"com.bizevent.single",



"id":"2",



"orderItemsProductIDs":"[\"PR-102002002\",\"QZ-123232\"]",



"paymentType":"paypal",



"plannedDeliveryDate":"01.01.2024",



"total":234



}
```

Example payload in pure JSON for ingesting multiple events

```
[



{



"id":"1",



"paymentType":"paypal",



"plannedDeliveryDate":"01.01.2021",



"event.provider":"custom.provider.array",



"total":234,



"customer":{



"firstName":"John",



"lastName":"Doe"



},



"orderItemsProductIDs":[



"PR-102002002",



"QZ-123232"



]



},



{



"id":"1",



"paymentType":"paypal",



"plannedDeliveryDate":"01.01.2021",



"event.provider":"custom.provider.array",



"total":234,



"customer":{



"firstName":"John",



"lastName":"Doe"



},



"orderItemsProductIDs":[



"PR-102002002",



"QZ-123232"



]



}



]
```

#### CloudEvents

In the [CloudEvents standardï»¿](https://dt-url.net/gi02yvo), the required fields below can be enriched by additional data fields.

* `specversion`
* `source`âAutomatically converted to `event.provider`
* `type`âAutomatically converted to `event.type`
* `id`âAutomatically converted to `event.id`

Provide additional data in the `data` object fields. In the example below, the additional data is `paymentId`, `orderId`, and `person`.

Example CloudEvents payload

```
{



"specversion": "1.0",



"id": "8d8c6e5d-829d-4629-86fb-23cda5496fa9",



"source": "ba.dt.local",



"type": "com.dynatrace.business",



"time": "2023-08-07T07:07:13.532Z",



"data": {



"paymentId": "110",



"orderId": "5117",



"person": {



"firstName": "Max",



"lastName": "Meyer"



}



}



}
```

#### CloudEvents batch

The CloudEvents batch mode enables the ingestion of multiple events at a time. As in the previous example, required fields can be enriched by additional data fields, such as `paymentId` and `orderId`.

Example CloudEvents batch payload

```
[



{



"specversion": "1.0",



"id": "8d8c6e5d-829d-4629-86fb-23cda5496fa9",



"source": "ba.dt.local",



"type": "com.dynatrace.business",



"time": "2023-08-07T07:07:13.532Z",



"data": {



"paymentId": "110",



"orderId": "5717"



}



},



{



"specversion": "1.0",



"id": "37d9df29-ba57-4526-8ac7-6baa6ccbc9c4",



"source": "ba.dt.local",



"type": "com.dynatrace.business",



"time": "2023-08-07T07:07:14.532Z",



"data": {



"paymentId": "111",



"orderId": "5718"



}



}



]
```

### Business event enrichment

When you report business events, Dynatrace enriches them by adding more context. For example, Dynatrace adds information about your application, geolocation, device, and more. This pertains only to data ingested via the Business events API.

For details, see [Business event enrichment](/docs/observe/business-observability/bo-events-enrichment "Check what properties Dynatrace automatically adds to reported business events.").

### Practice using the Business events API endpoint

If you have a Dynatrace SaaS environment, import our **Hands-on Business Observability** notebook to step through the process of ingesting business events using the API endpoint. The Notebook guides you through configuring and testing the API using a public data source.

1. Copy and save the JSON file for the notebook.

   Getting started with Business Observability - External data sources JSON file

   ```
   {



   "version": "5",



   "defaultTimeframe": {



   "from": "now-2h",



   "to": "now"



   },



   "sections": [



   {



   "id": "4becc036-acd5-4985-ae63-e6cad53f2000",



   "type": "markdown",



   "markdown": "# Hands on Business Observability\n\n### What you will learn\n- [x] How to ingest \"[business events](https://www.dynatrace.com/support/help/platform-modules/business-observability/bo-api-ingest)\" (a.k.a generic data) into Dynatrace without OneAgent\n- [x] What are the prerequisites\n- [x] Importance of `event.provider` and `event.type` attributes\n- [x] Automating the ingest via AutomationEngine\n- [x] Enriching the data\n- [x] Analytics, Dashboards, and more\n\nReference:\n- Online documentation: [Business Observability](https://www.dynatrace.com/support/help/platform-modules/business-observability)\n- [Basic concepts of Dynatrace Business Observability](https://www.dynatrace.com/support/help/shortlink/bo-basic-concepts)"



   },



   {



   "id": "d7488bcb-d354-46b0-b2b7-53f35ec4753d",



   "type": "markdown",



   "markdown": "## What you need to know first...\n\n### Business event\n\nAn event is an action or occurrence that takes place within a system or a \"service\". The system ***produces a signal*** when an event occurs. An event becomes a business event when it generates business-grade data.\n\n### Business events can come from many sources\n- Non-OneAgent sources, example [CloudEvents](https://cloudevents.io/)\n- OneAgent, stored in the `bizevents` Grail table as bizevents data object\n- RUM data, stored in the `bizevents` Grail table as bizevents data object\n- Generic data ingest via bizevents API endpoint, stored in the `bizevents` Grail table as bizevents data object\n- Logs, currently stored in the `logs` table, as logs data object. Enhancements coming in future to allow log data sources to be stored as bizevents data object.\n\n### Why is this important?\n\nDynatrace prioritizes business events separately from observability data to ensure business-grade data.\n\n### What is Business-grade data?\n\nPrecise data that doesn't rely on samples to report baselines, identify trends, or alert on anomalies with statistical accuracy.\n\nBusiness-grade data is often required for business decisions and reporting where ***precision is critical***.\n\nContrast this with typical IT reporting which achieves statistical accuracy through sampling and extrapolation."



   },



   {



   "id": "199c0513-b8b8-4305-9231-a6fb49e70d75",



   "type": "markdown",



   "markdown": "## Listen and watch\n--- \n##### Prerequisites\n- OAuth2 bearer token for API access\n- [JSON format in request body](https://www.dynatrace.com/support/help/shortlink/bo-api-ingest#request-body) for BizEvents API endpoint\n\n##### Note\nOAuth2 will be key to accessing more and more critical aspects of the new platform. It is important for you to know how to do this.\n\nHowever, this is somewhat like a \"do once and forget\" task, so it is not often executed and thus many will lack practice.\n\nDo practice and exchange tips with your peers.\n\n##### Resources\nWhat is OAuth?\n- [Wikipeidea](https://en.wikipedia.org/wiki/OAuth)\n- [Youtube](https://www.youtube.com/watch?v=LD3NCUP5hW4)\n\nHow is it set in Dynatrace?\n- [Online docs](https://www.dynatrace.com/support/help/dynatrace-api/basics/dynatrace-api-authentication/account-api-authentication)\n\n##### Demo\n- Creating OAuth2 bearer token\n- Take note of the unique properties of a bearer token\n- Building the API query and using the bearer token\n- Example of data ingest via your favourite API tool"



   },



   {



   "id": "b8ca400b-f7a0-4b1c-8b0a-b0489cc3d732",



   "type": "markdown",



   "markdown": "## Follow and do\n\nOverview of the tasks\n1. Automate\n1. Build query\n1. Enriching the data\n1. Build dashboards etc."



   },



   {



   "id": "92d1de22-7f09-4385-88cf-dab3aac40ffd",



   "type": "markdown",



   "markdown": "### Step 1: Automate \n--- \n\n- Create a workflow to ingest data from a public API source\n  - Use on-demand trigger\n- Create a task to get the bearer token\n  - Copy the payload from the example workflow or from here\n  ```\n  grant_type=client_credentials&client_id=[CLIENTID goes here]client_secret=[CLIENTSECRET goes here]\n  ```\n  - Replace the strings in square brackets with your own `clientid` and `clientsecret`\n- Create a task to get the data from a public API\n  - Use `Http request`\n  - Enter the external API URL in the `URL field`\n- Create a task to insert the data via the `bizevents` Dynatrace API\n  - Use `Http request`\n  - payload\n    - enter `{{result(\"name_of_task\").json.<replace with the data structure of your JSON results> }}`\n  - ensure you set the headers\n    - Content-Type\n      - Pure JSON: no mandatory fields. Content-Type `application/json`\n      - CloudEvent: Mandatory fields are Specversion, Source, Type, Id. Content-Type `application/cloudevent+json`\n      - CloudEvent batch (batch ingest of events): Mandatory fields as above. Content-Type `cloudevent-batch+json`\n    - Authorization\n      - `Bearer {{ result(\"get_bearer_token\").json.access_token }}`\n\n##### Discussion points\n- Use `Run Javascript task` if need to do advanced \"computation\" or \"logic\" with the data. (Industry term - \"transform the data\".)\n- [Jinja templating engine](https://www.dynatrace.com/support/help/platform-modules/cloud-automation/workflows/reference) can also be used within `Http request task` to \"transform\" the payload data"



   },



   {



   "id": "15ebff0c-6205-48e6-b1cc-faccbed6426b",



   "type": "markdown",



   "markdown": "### Step 2: Validate the data\n--- \n\nLet's start by validating what data we have first.\n\n##### Discussion points\n- What do you notice when you query the data? Is it easy to find out which dataset is yours?\n- What if you would like to define how your data will be processed further?"



   },



   {



   "id": "0786560c-8c2d-4583-9550-31309840fe9a",



   "type": "dql",



   "showTitle": false,



   "state": {



   "input": {



   "value": "fetch bizevents",



   "timeframe": {



   "from": "now-2h",



   "to": "now"



   }



   },



   "state": "idle",



   "visualizationSettings": {



   "chartSettings": {



   "gapPolicy": "connect",



   "circleChartSettings": {



   "groupingThresholdType": "absolute"



   }



   },



   "singleValue": {



   "showLabel": true,



   "label": "",



   "autoscale": true



   },



   "table": {



   "rowDensity": "condensed",



   "enableSparklines": false,



   "hiddenColumns": [],



   "lineWrapIds": [],



   "firstVisibleRowIndex": 0,



   "columnWidths": {}



   }



   }



   },



   "davisAnalytics": {



   "analyzerComponentState": {



   "resultState": {}



   }



   }



   },



   {



   "id": "85b10d2c-b709-4b34-8e71-d03b0c896738",



   "type": "markdown",



   "markdown": "### Step 3: Enriching the data\n---\n\n- [Predefined attributes](https://www.dynatrace.com/support/help/shortlink/ba-business-events-enrichment) automated if using OneAgent and RUM, further enrichment can be done\n- If using API, some important attributes require configuration\n- This is done via [Business events processing](https://www.dynatrace.com/support/help/shortlink/ba-business-events-processing)\n\nWe will enrich the incoming data set with the following 2 attributes\n- `event.provider`\n- `event.type`\n\n###### 3.1\nGo to Settings > Business Analytics > Ingest Pipeline > Processing\n\n###### 3.2\nGive the rule a name and add matcher\n\n###### 3.3\nAdd the necessary DQL under `Processor definition`.\n\n###### 3.4\nTest the rule and remember to Save\n\n###### 3.5\n- Trigger the workflow and query Dynatrace again, now with the `event.provider` attribute as a filter.\n- What do you notice now after you query for the data?"



   },



   {



   "id": "1416beec-b33f-4e5c-a054-6cb40897c7a0",



   "type": "markdown",



   "markdown": "### Step 4: Build dashboards etc.\n---\n\nNow that we have organized the data, let's see... what should we do with the data?\n\nEven if we would like to build dashboards, we need an objective, won't we?\n\nFor a start let's answer this question: ***\"Which are the most popular rental spots in the city?\"***\n\nBuild a dashboard that can visualize this data. Use category chats and switch it to horizontal. Play around with the visualization!\n\n##### Discussion points\n- From here, let's see what else can we do with the data..."



   },



   {



   "id": "1db4f97e-4ed9-4bf4-8215-885ccef7bef6",



   "type": "dql",



   "showTitle": false,



   "state": {



   "input": {



   "value": "fetch bizevents\n| summarize popular = count(), by: {RENT_NM}\n| filter popular < 1000\n| sort popular desc\n| limit 20",



   "timeframe": {



   "from": "now-2h",



   "to": "now"



   }



   },



   "state": "idle",



   "visualizationSettings": {



   "chartSettings": {



   "gapPolicy": "connect",



   "circleChartSettings": {



   "groupingThresholdType": "absolute"



   }



   },



   "singleValue": {



   "showLabel": true,



   "label": "",



   "autoscale": true



   },



   "table": {



   "rowDensity": "condensed",



   "enableSparklines": false,



   "hiddenColumns": [],



   "lineWrapIds": [],



   "firstVisibleRowIndex": 0,



   "columnWidths": {}



   }



   }



   },



   "davisAnalytics": {



   "analyzerComponentState": {



   "resultState": {}



   }



   }



   }



   ]



   }
   ```
2. In Dynatrace, go to **Notebooks** ![Notebooks](https://dt-cdn.net/images/notebooks-768-046137830a.webp "Notebooks").
3. Expand ![Open Dashboards panel](https://dt-cdn.net/images/dashboards-app-dashboards-panel-open-6c03b43117.svg "Open Dashboards panel") the sidebar.
4. Select **Upload** to import the notebook.
5. Follow the tutorial instructions in the notebook.

## DQL queries via API

The Dynatrace Grail - DQL Query API enables third-party applications to retrieve data from Grail by executing DQL queries via API. You can only query data, not ingest it via this API.

To view Swagger documentation and try out this API in the latest Dynatrace

1. Search for "API" or "Dynatrace API."
2. From the dropdown list in the top-right corner of the page, select **Grail - DQL Query**.
3. Expand **Query Execution** to view its methods.

To run a query and to retrieve its result, you need to make two requests via separate methods.

1. To query Grail, execute the [POST `/query:execute`](#query-execute) request.
2. To retrieve the result, execute the [GET `/query:poll`](#query-poll) request.

### POST /query:execute method

* Endpoint URLâ`https://{your-environment-id}.apps.dynatrace.com/platform/storage/query/v1/query:execute`
* MethodâPOST
* Authenticationâ[OAuth](#oauth)
* `Content-Type` headerâ`application/json`
* Request bodyâ`{your DQL query}`

#### Request body example

```
{



"query": "fetch bizevents | summarize Count()"



}
```

#### Response body example

You will need to use the `requestToken` value from this response in the [`/query:poll`](#query-poll) request.

```
{



"state": "SUCCEEDED",



"requestToken": " +kuSj8qvRvq64GkG5CEHag==",



"progress": 100



}
```

### GET /query:poll method

* Endpoint URLâ`https://{your-environment-id}.apps.dynatrace.com/platform/storage/query/v1/query:poll`
* MethodâGET
* Authenticationâ[OAuth](#oauth)
* `Content-Type` headerâ`application/json` (Pure JSON)
* GET parameterâ`request-token`, used to identify the executed POST query

#### Request URL example

```
https://mySampleEnv.apps.dynatrace.com/platform/storage/query/v1/query:poll?request-token=%2BkuSj8qvRvq64GkG5CEHag%3D%3D
```

#### Response body example

```
{



"state": "SUCCEEDED",



"progress": 100,



"result": {



"records": [



{



"count()": "0"



}



],



"types": [



{



"indexRange": [



0,



0



],



"mappings": {



"count()": {



"type": "long"



}



}



}



],



"metadata": {



"grail": {



"canonicalQuery": "fetch bizevents\n| summarize count()",



"timezone": "Z",



"query": "fetch bizevents | summarize Count()",



"scannedRecords": 0,



"dqlVersion": "V1_0",



"scannedBytes": 0,



"analysisTimeframe": {



"start": "2023-07-11T07:28:11.068767000Z",



"end": "2023-07-11T09:28:11.068767000Z"



},



"locale": "",



"executionTimeMilliseconds": 4155,



"notifications": [],



"queryId": "fa4b928f-caaf-46fa-bae0-6906e421076a",



"sampled": false



}



}



}



}
```

## Authentication

* The Business events API uses [access-token authentication](#access-token) for its `bizevents/ingest` endpoint URL in the previous Dynatrace (`https://{your-environment-id}.live.dynatrace.com/api/v2/bizevents/ingest`).

  If you are working in the latest Dynatrace, you can access this endpoint via `https://{your-environment-id}.apps.dynatrace.com/platform/classic/environment-api/v2/bizevents/ingest`. This URL only supports [OAuth](#oauth) for API access.
* The Grail - DQL Query API uses [OAuth](#oauth) for API access.

### Access token

To create a new access token for the `/bizevents/ingest` endpoint of the Business events API, follow the procedure described in [Dynatrace API - Tokens and authentication](/docs/dynatrace-api/basics/dynatrace-api-authentication#create-token "Find out how to get authenticated to use the Dynatrace API.").

You need to select the **Ingest bizevents** token scope.

To authenticate a call, attach the token to the `Authorization` HTTP header preceding the `Api-Token` realm.

```
--header 'Authorization: Api-Token dt0c01.abc123.abcdefjhij1234567890'
```

### Set up and use OAuth for API access

For both the Business events API and the Grail - DQL Query API, you first need a valid bearer token for authentication, which you request by setting up OAuth for API access.

[![Step 1](https://dt-cdn.net/images/step-1-086e22066c.svg "Step 1")

**Create an OAuth client**](/docs/observe/business-observability/bo-api-ingest#oauth-client "Set up authentication for and ingest business events via API.")[![Step 2](https://dt-cdn.net/images/step-2-1a1384627e.svg "Step 2")

**Create an IAM policy**](/docs/observe/business-observability/bo-api-ingest#iam-policy "Set up authentication for and ingest business events via API.")[![Step 3](https://dt-cdn.net/images/step-3-350cf6c19a.svg "Step 3")

**Bind the policy to a group**](/docs/observe/business-observability/bo-api-ingest#bind-policy "Set up authentication for and ingest business events via API.")[![Step 4](https://dt-cdn.net/images/step-4-3f89d67d41.svg "Step 4")

**Obtain a bearer token**](/docs/observe/business-observability/bo-api-ingest#obtain-token "Set up authentication for and ingest business events via API.")[![Step 5](https://dt-cdn.net/images/step-5-2de312b50f.svg "Step 5")

**Authenticate**](/docs/observe/business-observability/bo-api-ingest#authenticate "Set up authentication for and ingest business events via API.")

#### Step 1 Create an OAuth client

You can create new OAuth clients on the **Account Management** page in Dynatrace.

1. Go to [**Account Management**ï»¿](https://myaccount.dynatrace.com/). If you have more than one account, select the account you want to manage.
2. From the top menu bar, select **Identity & access management** > **OAuth clients**.
3. Select **Create client**.
4. Provide the email address of the user who will own the client.
5. Provide a description for the new client.
6. Ensure that your client has the required permissions by selecting one or more options during client setup. For reading and writing business events, you require:

   * Read business events: (`storage:bizevents:read`)
   * Read Grail buckets: (`storage:buckets:read`)
   * Write/edit events: (`storage:events:write`) (only necessary for ingesting data via the [Business events API](#ingest-endpoint))
7. Select **Create client**.
8. Save the generated client secret to a password manager for future use. You will also require the generated client ID when [obtaining a bearer token](#obtain-token).

   The client secret is only displayed once upon creation, after which it's stored encrypted and can't be revealed.

#### Step 2 Create an IAM policy

To use the OAuth client, you need to ensure that the right IAM policy is assigned to your user.

To set up the policy

1. Go to [**Account Management**ï»¿](https://myaccount.dynatrace.com/). If you have more than one account, select the account you want to manage.
2. From the top menu bar, select **Identity & access management** > **Policy management**.
3. Select **Create policy** and set up the policy name and description. You'll need this information when you [bind this policy to a user group](#bind-policy) later. Read more about policy management in [Manage IAM policies](/docs/manage/identity-access-management/permission-management/manage-user-permissions-policies/iam-policy-mgt "Create, edit, copy, and delete IAM policies for managing Dynatrace user permissions.").
4. Add the following policy statements for writing and querying business events.

   ```
   ALLOW storage:buckets:read WHERE storage:table-name = "bizevents";



   ALLOW storage:bizevents:read;



   ALLOW storage:events:write;
   ```
5. Select **Create policy**.

#### Step 3 Bind the policy to a group

You can assign the [policy you created](#iam-policy) to a group that your user belongs to or add your user to a new group. See [Working with policies](/docs/manage/identity-access-management/permission-management/manage-user-permissions-policies#add-or-remove "Working with policies") for instructions on binding policies to groups.

#### Step 4 Obtain a bearer token

You can use the [OAuth client](#oauth-client) to obtain an OAuth 2.0 bearer token. Use the information below send a request to the bearer token URL. Use the token you receive in the response to [authenticate](#authenticate) to the [Business events API](#ingest-endpoint) or the [Grail - DQL Query API](#dql-via-api).

* Endpoint URL for requesting a tokenâ`https://sso.dynatrace.com/sso/oauth2/token`
* MethodâPOST
* `Content-Type` headerâ`application/x-www-form-urlencoded`
* Keys/parametersâYou can send the following keys or parameters in the request body. Make sure to URL encode all values.

  Key/Parameter

  Value

  Required

  `grant_type`

  `client_credentials`

  Required

  `client_id`

  `dt0s02.****`âThis is the client ID generated when you created an [OAuth client](#oauth-client).

  Required

  `client_secret`

  `dt0s02.***.****`âThis is the client secret generated when you created an [OAuth client](#oauth-client).

  Required

  `scope`

  A list of required scopes separated by a space, for example, `storage:bizevents:read storage:buckets:read storage:events:write`

  You can assign multiple scopes to a single token, or you can generate several tokens, each with different access levels and use them accordinglyâcheck your organization's security policies for the best practice.

  Required

  `resource`

  `urn:dtaccount:{your-account-UUID}`

  Optional if not defined in the client

  Required if defined in the client, with the same value as in the client

Example API request

Request headers

```
POST /sso/oauth2/token HTTP/2



Host: sso.dynatrace.com



content-type: application/x-www-form-urlencoded
```

Request body

```
grant_type=client_credentials&client_id=dt0s02.****&client_secret=dt0s02.***.****&scope=storage%3Abizevents%3Aread+storage%3Aevents%3Awrite+storage%3Abuckets%3Aread
```

Example API response

```
{



"scope": "storage:bizevents:read storage:events:write storage:buckets:read",



"token_type":"Bearer",



"expires_in":300,



"access_token":"eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NiIsImtpZCI6IjEifQ.eyJzdWIiOiI2YWUwMmFmNC01NmIwLTRhM2MtODI0OS1mYjc2ZDgyOTYwYTQiLCJyZXMiOiJ1cm46ZHRhY2NvdW50OjJiMjM2NDM4LTJhMzEtNDg3Mi04NjU1LWJjYTRmYThhZjkxNyIsIl9jbGFpbV9uYW1lcyI6eyJncm91cHMiOiIwIn0sInByZWZlcnJlZF91c2VybmFtZSI6ImRhbmllbC5tYXJzY2huaWdAZHluYXRyYWNlLmNvbSIsImd0IjoiY2MiLCJpbnQiOnRydWUsImF1ZCI6ImR0MHMwMi4zWkJZWkhWVCIsInNjb3BlIjoiYWNjb3VudC1pZG0tcmVhZCBhY2NvdW50LWlkbS13cml0ZSBhY2NvdW50LWVudi1yZWFkIGFjY291bnQtZW52LXdyaXRlIGFjY291bnQtdWFjLXJlYWQgYWNjb3VudC11YWMtd3JpdGUgaWFtLXBvbGljaWVzLW1hbmFnZW1lbnQgaWFtOnBvbGljaWVzOndyaXRlIGlhbTpwb2xpY2llczpyZWFkIGlhbTpiaW5kaW5nczp3cml0ZSBpYW06YmluZGluZ3M6cmVhZCBpYW06ZWZmZWN0aXZlLXBlcm1pc3Npb25zOnJlYWQgc3RvcmFnZTpiaXpldmVudHM6cmVhZCBzdG9yYWdlOmV2ZW50czp3cml0ZSIsIl9jbGFpbV9zb3VyY2VzIjp7IjAiOnsiZW5kcG9pbnQiOm51bGx9fSwiZXhwIjoxNjkxNjE1NTcxLCJpYXQiOjE2OTE2MTUyNzEsImp0aSI6ImJmNDZhNmFhLTRjZmItNDA1MS05ZmUzLTMzZTljMjc0OTdhZSIsImVtYWlsIjoiZGFuaWVsLm1hcnNjaG5pZ0BkeW5hdHJhY2UuY29tIn0.x3_dUuOmdqcrrsfuRyiUwKOOFSJ30fAQnpKGbJTNGy8N3qspKDO0OC36Z0GILc275bTsLY3fARzRQNBOW1Okmg",



"resource":"urn:dtaccount:2b236438-2a31-4872-8655-bca4fa8af917"



}
```

#### Step 5 Authenticate

To authenticate a call, attach the token to the `Authorization` HTTP header preceding the `Bearer` realm.

```
--header 'Authorization: Bearer abcdefjhij1234567890'
```


---


## Source: bo-basic-concepts.md


---
title: Basic concepts of Dynatrace Business Observability
source: https://www.dynatrace.com/docs/observe/business-observability/bo-basic-concepts
scraped: 2026-02-17T21:21:42.597869
---

# Basic concepts of Dynatrace Business Observability

# Basic concepts of Dynatrace Business Observability

* Latest Dynatrace
* Explanation
* 3-min read
* Published Oct 24, 2022

Business events are an important concept and central to Dynatrace Business Observability. Business events generate business-grade data to enable important use cases.

## Business event

An event is an action or occurrence that takes place within a system or a service, such as when a customer completes a satisfaction survey or when a user selects a subscribe button. The system produces a signal when an event occurs.
An event becomes a **business** event when it generates **business-grade data**. Dynatrace prioritizes business events separately from observability data to ensure business-grade data.

## Business-grade data

Business-grade data is precise data that doesn't rely on samples to report baselines, identify trends, or alert on anomalies with statistical accuracy. It relies on complete and accurate data to inform businesses about system performance and anomalies in real time. Business events deliver complete and precise data to support demanding business use cases, and to support IT use cases where full accuracy is required.

Business-grade data is:

* **Complete**  
  All events are captured without sampling.
* **Fully accessible**  
  All events can be queried without extrapolation.
* **Real-time**  
  Events are available immediately after they occur.
* **Unconstrained**  
  There is no limit to the number of events generated.
* **Granular**  
  Events are not aggregated as they age.
* **Long-term**  
  The event retention period is up to ten years.
* **Topology-aware**  
  Events are stored with topology context.
* **Available from multiple sources**  
  Events captured include those generated by OneAgent, RUM, logs, and external sources.
* **Unified**  
  Events are stored in a single data store (Grail) and accessed through a single query language, [DQL](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language.").

Business-grade data is often required for business decisions and reporting where precision is critical. Contrast this with typical IT reporting which achieves statistical accuracy through sampling and extrapolation.

## Business event use cases

* Business event data can be used to track financial information such as sales revenue and profit margins. Seasonal and longer-term trends can be used to anticipate demand. Seasonal trends can be easily transformed into long-term trends without extrapolation, thanks to a ten-year retention period.
* Business events can be used to track complex business processes such as order management and fulfillment, to extract and segment financial details from a payment gateway, or to enrich Dynatrace data with external business data such as user feedback derived through voice of customer (VoC) solutions.
* Business events can, for example, be used by a car manufacturer to better understand customer journeys within online car configurators, even when journeys span multiple weeks or months and involve multiple touchpoints. Business leaders can identify where users get stuck, experiment with approaches to improve goal completion, and better target specific audience demographics.

## Business event dataflow

The business event dataflow in Dynatrace has three stages:

1. **Capture**

   Business event data can be ingested from OneAgent, the RUM JavaScript API, and external sources.
2. **Process**

   Pipelines process incoming business events to improve analysis and reporting. Processing options include filtering, parsing, enriching, transforming, or assigning a retention period.
3. **Analyze**

   Data stored in Grailâ¢ can be explored using the Dynatrace Query Language, [DQL](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language.") to discover patterns, identify anomalies and outliers, report on trends, and more. DQL queries are also used to build dashboards, charts, metrics, and reports. You can use DQL to export selected data to external tools.

## Related topics

* [DDUs for business events](/docs/license/monitoring-consumption-classic/davis-data-units/ddus-for-business-events "Understand how the volume of DDU consumption is calculated for business events.")


---


## Source: bo-bucket-assignment.md


---
title: Business event bucket assignment via classic pipeline
source: https://www.dynatrace.com/docs/observe/business-observability/bo-event-processing/bo-bucket-assignment
scraped: 2026-02-17T21:21:40.077518
---

# Business event bucket assignment via classic pipeline

# Business event bucket assignment via classic pipeline

* Latest Dynatrace
* Reference
* 2-min read
* Updated on Nov 28, 2025

Business events can be stored in buckets that can have different retention periods. You create rules with matcher-specific [DQL](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language.") queries to assign matching business events to a bucket. The default retention period for a built-in business events bucket (`default_bizevents`) is 35 days. You can also create [custom buckets](/docs/platform/grail/organize-data/assign-permissions-in-grail#custom-grail-buckets "Find out how to assign permissions to buckets and tables in Grail.") with a specific retention period.

For custom buckets, the possible retention period ranges from 1 day to 10 years, with an additional week.

Business events can be stored in different buckets that determine the retention time and access permissions. You can create rules with matcher-specific DQL queries to assign matching business events to a bucket. By default, every incoming business event is stored in the default bucket with a 35-day retention period. Longer and shorter periods can also be set.

## Choose the retention period

1. Go to **Settings** > **Business Observability** > **Ingest Pipeline** > **Bucket assignment**.
2. In the **Business event bucket assignment**, select **Add rule** and name your rule.
3. Select the **Bucket**.
4. Add a **Matcher** to your rule by typing or pasting your [matcher-specific DQL query](/docs/analyze-explore-automate/logs/lma-classic-log-processing#dql-functions "Utilize log processing rules to reshape incoming log data for better understanding, analysis, or further transformation."). Events that match your rule will be assigned to your selected bucket. If no rules match, events will be assigned to the default bucket.
5. Select **Save changes**.

## Matcher examples

* If you needed to add only one event type (for example, `com.easytrade.buy-assets`), the matcher would be:

```
matchesValue(event.type, "com.easytrade.buy-assets")
```

* For two event types within the same event provider, the matcher would be:

```
matchesValue(event.type, "com.easytrade.buy-assets") or matchesValue(event.type, "com.  easytrade.sell-assets")
```

* In this use case, however, you need to take all event types under the EasyTrade event provider, so it is sufficient just to use:

```
matchesValue(event.provider, "www.easytrade.com")
```


---


## Source: bo-metric-extraction.md


---
title: Business event metric extraction via classic pipeline
source: https://www.dynatrace.com/docs/observe/business-observability/bo-event-processing/bo-metric-extraction
scraped: 2026-02-17T05:02:09.215545
---

# Business event metric extraction via classic pipeline

# Business event metric extraction via classic pipeline

* Latest Dynatrace
* How-to guide
* 3-min read
* Updated on Sep 06, 2023

With business event metric extraction via the classic pipeline, you can create your own business metrics. It enables you to:

* Create custom alerts, such as when a certain value surges or drops. Alerts can be based on attribute values or specific business event occurrences.
* Reduce your DDU consumption and lower your costs.

You need to create your business metrics before ingesting business event data.

## Configure metric extraction

To add a business event metric

1. Go to **Settings** > **Business Observability** > **Metric extraction**.
2. Select **Add business event metric** and name your metric by adding a metric **Key** starting with the `bizevents.` prefix (for example, `bizevents.EasyTrade.TradingVolume`).
3. Add a **Matcher** to your rule by pasting your [matcher-specific DQL query](/docs/analyze-explore-automate/logs/lma-classic-log-processing/lma-log-processing-matcher "Examine specific DQL functions and logical operators for log processing."). In the above example, to calculate your trading volume metric, you need to extract only buy transactions, so the matcher query is as follows.

   ```
   matchesValue(event.type, "com.easytrade.buy-assets")
   ```
4. Choose the **Measure** on which your metric will be based. There are two options.

   * **Occurrence of business events records**âa count of events that match your DQL query
   * **Attribute value**âa collection of measures for the attribute value of business events that match your DQL query

     Specify your attribute name in the **Attribute** field. Attribute name matching is not case sensitive. For example, if you choose `Trading_Volume`, the specified attribute name for metric extraction should be `trading_volume`. However, be sure to use the exact attribute name of your business event.
5. Select **Add dimension**. Adding dimensions allows you to split the business event occurrences by a specific business event attribute such as a hostname. If the attribute contains more than one value, the first attribute value acts as the metric dimension. The maximum number of dimensions is 50. Be sure to use the exact attribute name of your business event.
6. Select **Save changes**.

## Visualize metrics

You can extract and visualize your metrics to use them further in your analysis. You can also create metric-based alerts tailored to your needs.

### Display metrics in Data Explorer

To display business event metrics

1. Go to **Data Explorer**.
2. Find your metric in the search window, select **Run query**, and display the results.

   You can also:

   * Visualize your metric on a classic dashboard by selecting **Pin to dashboard**.
   * Export your data to a CSV file.
   * Share a link.
   * Copy the request.

See the example visualization below.

![Visualization of business events metric, bizevents.EasyTrade.TradingVolume](https://dt-cdn.net/images/business-event-metric-display-2806-d2f55d9cc7.png)

### Display metrics in Notebooks Notebooks

You can also explore custom metrics based on business events in Grail, for example, by using the DQL [`timeseries` command](/docs/platform/grail/dynatrace-query-language/commands/metric-commands#timeseries "DQL metric commands"). The following is an example DQL `timeseries` query against the `bizevents.easyTrade.TradingVolume` metric.

```
timeseries avg(bizevents.easyTrade.TradingVolume), alias:avgTradingVolume, interval:1d, from:now()-30d, to:now()
```

Run this query in [Notebooks](/docs/analyze-explore-automate/dashboards-and-notebooks/notebooks "Analyze, visualize, and share insights from your observability dataâall in one collaborative, customizable workspace.") (**Query Grail** > **Run query**) and view results using the recommended **Line chart** visualization option.

![Query Business Observability custom metrics through DQL with timeseries in Notebooks](https://dt-cdn.net/images/ba-event-metric-query-notebooks-2122-6c3ead0aef.webp)

### Create alerts with metrics

To create alerts based on business event metrics

1. Go to **Settings** > **Anomaly detection** > **Metric events**.
2. Select **Add metric event** and create a custom event where **Type** is **Metric selector**. This is the [metric key defined earlier](#configure), for example, `bizevents.EasyTrade.TradingVolume`.

   Metric events based on business events are only supported with metric selector queries. See [Metric selector events](/docs/dynatrace-intelligence/anomaly-detection/metric-events/metric-selector-events#metricselectorevent "Learn about metric events based on a metric selector.") for more information.

### Unrecognized timestamp handling

If the event timestamp doesn't fall within the allowed [range](/docs/ingest-from/extend-dynatrace/extend-metrics/reference/metric-ingestion-protocol#payload "Learn how the data ingestion protocol for Dynatrace Metrics API works."), your metric is replaced in Grail by a metric with the `.failed` suffix added to the metric key. This new metric will have a recent timestamp, `(now())` and the dimension that would have been attached to the metric you wanted to extract. You can also visualize this metric in [Data Explorer](#data-explorer) or [Notebooks](#notebooks).

## Related topics

* [Metric ingestion protocol](/docs/ingest-from/extend-dynatrace/extend-metrics/reference/metric-ingestion-protocol "Learn how the data ingestion protocol for Dynatrace Metrics API works.")


---


## Source: bo-processing-classic-pipeline.md


---
title: Business event processing via classic pipeline
source: https://www.dynatrace.com/docs/observe/business-observability/bo-event-processing/bo-processing-classic-pipeline
scraped: 2026-02-17T21:21:38.868525
---

# Business event processing via classic pipeline

# Business event processing via classic pipeline

* Latest Dynatrace
* Tutorial
* 5-min read
* Published Oct 24, 2022

Business event processing via the classic pipeline allows you to define how your data will be processed further.

* You can create a processing rule that transforms incoming events.
* You can also specify the retention time for your events by assigning them to buckets.

If you create multiple rules, the rules are performed in the order in which you have defined them.

We recommend utilizing business event processing with [OpenPipeline](/docs/platform/openpipeline "Scale Dynatrace platform data handling with Dynatrace OpenPipeline.") as a scalable, powerful solution to manage and process business events. If you don't have access to OpenPipeline, use the classic business event processing pipeline.

## Configure business event processing rules

To configure business event processing rules

1. Go to **Settings** > **Business Observability** > **Ingest Pipeline** > **Processing**.
2. Select **Add rule** and name your rule.
3. Add a **Matcher** to your rule by pasting your [matcher-specific DQL query](/docs/observe/business-observability/bo-event-processing/bo-events-processing-matcher "This is the DQL matcher in events in the classic pipeline .").

   Matching based on previous rules is not supported

   The matcher operates on the initial data set before applying any processing rules. Matching records modified by preceding rules is not supported. For example, the modified value of an attribute in rule 1 cannot be used as a matching value in rule 2.
4. Select **Add item** to choose the fields you will transform via the processor definition. Determine the data types and names for your fields. You can choose if the rule is **Optional**, **Is Array**, or **Read-only**.

   * **Optional**: If a transformation field is marked as optional, the transformation can happen even if the field is not provided. But if a transformation field is marked as mandatory (not optional) , the transformation occures only if the field is present in the incoming data.
   * **Is Array**: Needs to be selected when a field contains multiple values, and you intend to work with specific values.
   * **Read-Only**: Cannot be changed during transformation.
5. Add a **Processor definition**.
   Processor definitions are instructions that tell Dynatrace how to transform the data youâve selected in the matcher DQL query.
   It is created using [processing commands](/docs/analyze-explore-automate/logs/lma-classic-log-processing/lma-log-processing-commands "Explore scenarios of how to use log processing commands in Dynatrace powered by Grail."), [functions](/docs/analyze-explore-automate/logs/lma-classic-log-processing/lma-log-processing-functions "Explore scenarios of how to use log processing functions in Dynatrace powered by Grail.") and [pattern matching](/docs/platform/grail/dynatrace-pattern-language "Use Dynatrace Pattern Language to describe patterns using matchers.") (Dynatrace Pattern Language) that allows you to add, modify or remove incoming records.
6. In the **Rule testing** section, paste an event sample and run the test.

   Your incoming data example needs to be provided in the JSON format.
7. Select **Save changes**.

## Business events processing examples

In the below examples, you can see how ingest pipeline processing can be used to meet your technical and administrative requirements.

* Example 1. Add a new calculated field to the pipeline.  
  As a user, you need to add a calculated dollar trading volume value to the pipeline.

  ```
  {



  "action":"buy",



  "accountId":6,



  "amount":10,



  "instrumentId":1,



  "price":157.025



  }
  ```

  + **Rule name**: Add field
  + **Matcher**: `matchesValue(action, "buy")`
  + **Transformation fields**:

  Name

  Type

  Optional

  is Array

  Read-only

  amount

  double

  false

  false

  true

  price

  double

  false

  false

  true

  + **Processor definition**: `FIELDS_ADD(trading_volume: price*amount)`
* Example 2. Mask your data.  
  You need to hide the CVV field on your credit card in an incoming JSON payload.

  ```
  {



  "action": "payment",



  "creditCardNumber":"5570001112223344",



  "valid":"12/27",



  "cvv":"001"



  }
  ```

  + **Rule name**: Mask field
  + **Matcher**: `matchesValue(action, "payment")`
  + **Transformation fields**:

  Name

  Type

  Optional

  is Array

  Read-only

  cvv

  string

  false

  false

  false

  + **Processor definition**: `FIELDS_ADD(cvv: SHA1(cvv))`
* Example 3. Drop event attribute.  
  You need to drop the birthdate field in an incoming JSON payload.

  ```
  {



  "action": "newUser",



  "firstName":"Frank",



  "lastName": "Underwud",



  "birthDate": "10.01.1967"



  }
  ```

  + **Rule name**: Drop field
  + **Matcher**: `matchesValue(action, "newUser")`
  + **Transformation fields**:

  Name

  Type

  Optional

  is Array

  Read-only

  birthDate

  string

  false

  false

  false

  + **Processor definition**: `FIELDS_REMOVE(birthDate)`
* Example 4. Parse nested JSON.  
  You need to parse attributes from a nested JSON in order to have them as top-level attributes in Grail.

  ```
  {



  "action":"sell",



  "details":{



  "accountId":6,



  "amount":10,



  "instrumentId":1,



  "price":157.025



  }



  }
  ```

  + **Rule name**: Parse field
  + **Matcher**: `matchesValue(action, "sell")`
  + **Transformation fields**:

  Name

  Type

  Optional

  is Array

  Read-only

  details

  string

  false

  false

  false

  + **Processor definition**: `PARSE(details,"JSON{INTEGER:accountId,INTEGER:amount,INTEGER:instrumentId,DOUBLE:price}(flat=true)") | FIELDS_REMOVE(details)`
* Example 5. Parse error message.

  In this example, you need to remove the voucher code from the error message field below to have the ability to count how often the same error message appears. The solution below leverages the [Dynatrace Pattern language](/docs/platform/grail/dynatrace-pattern-language "Use Dynatrace Pattern Language to describe patterns using matchers.") for parsing.

  ```
  {



  "timestamp":"2023-01-18T10:50:23.777000000Z",



  "cartId":"58583939",



  "error.message":"The voucher [XY-892940] is not valid!",



  "error.messageKey":"error.voucher "



  }
  ```

  + **Rule name**: Parse error message
  + **Matcher**: `matchesValue(error.messageKey, "error.voucher ")`
  + **Transformation fields**:

  Name

  Type;

  Optional

  is Array

  Read-only

  error.message

  string

  no

  no

  no

  + **Processor definition**:
    `FIELDS_ADD(final:REPLACE_PATTERN(error.message, "LD:p1 '[' LD:to_be_masked ']' LD:p2 ", "${p1}${p2}"))`
  + **Result**: `The voucher is not valid!`.


---


## Source: bo-security-context.md


---
title: Business events security context
source: https://www.dynatrace.com/docs/observe/business-observability/bo-event-processing/bo-security-context
scraped: 2026-02-17T21:24:48.862245
---

# Business events security context

# Business events security context

* Latest Dynatrace
* How-to guide
* 4-min read
* Updated on Nov 20, 2025

Dynatrace has a [permission model for Grail](/docs/platform/grail/organize-data/assign-permissions-in-grail "Find out how to assign permissions to buckets and tables in Grail."). This applies to all telemetry data, such as metrics, events, spans, and logs.

We recommend setting up permissions along organizational lines and deployment scopes. Suitable concepts include host groups, Kubernetes clusters, and Kubernetes namespaces. These attributes are typically available for all telemetry data ingested via Dynatrace collection methods like OneAgent, OpenTelemetry, or Kubernetes operator. Hence, you can use these attributes to enable [record-level permissions](/docs/platform/grail/organize-data/assign-permissions-in-grail#grail-permissions-table-record "Find out how to assign permissions to buckets and tables in Grail.").

For Kubernetes-based deployments, make sure Dynatrace Operator has [metadata enrichment](/docs/ingest-from/setup-on-k8s/guides/metadata-automation/metadata-enrichment "Metadata enrichment in the Dynatrace Operator adds context to Kubernetes pods by attaching relevant metadata to entities like pods, hosts, and processes for better observability.") enabled.

If you only require a basic permission concept, setting up bucket-level permissions is the best option. You can then route your data to the correct bucket in OpenPipeline by matching one of the mentioned deployment-relevant primary Grail fields.

For more control in Dynatrace, you can set up policy boundaries with more granular restrictions on a data level. By default, you can use the following attributes:

* `dt.host_group.id`
* `k8s.cluster.name`
* `k8s.namespace.name`
* Any other attribute listed in the [permission model](/docs/platform/grail/organize-data/assign-permissions-in-grail#grail-permissions-table-record "Find out how to assign permissions to buckets and tables in Grail.")

Dynatrace provides a comprehensive permission model for Grail that applies to all telemetry data-including metrics, logs, spans, and events.

## Set up security context

Dynatrace allows you to tweak your ingested business events data by adding a `dt.security_context` attribute to specific business events records. This enables you to set additional options, for example, permissions for individual records. See, [Permissions in Grail](/docs/platform/grail/organize-data/assign-permissions-in-grail "Find out how to assign permissions to buckets and tables in Grail.").

You can set those permissions for individual records on specific attribute values or automatically enriched entities in a business event (hosts, process groups, process group instances) when OneAgent captures data.

For basic use cases, it's best to have the permission boundaries following the deployment of your organization lines around hosts and process groups, or other attributes that can define such boundaries, such as the geographical location of the data.

To create a security context adjustment to your ingested business events data, you need to create rules that:

1. Filter the records that have the `dt.security_context` attribute added to them.  
   Go to ![Notebooks](https://dt-cdn.net/images/notebooks-768-046137830a.webp "Notebooks") **Notebooks** and create a DQL query using a [business events processing function](/docs/observe/business-observability/bo-event-processing/bo-events-processing-matcher "This is the DQL matcher in events in the classic pipeline ."). For example:

   ```
   fetch bizevents



   | filter matchesValue(geo.city.name, "Brussels")
   ```

   This allows you to filter the business events records to which you will want to add the `dt.security_context` attribute. Once satisfied with the query result, copy the business events processing function of the DQL query: `matchesValue(geo.city.name, "Brussels")`.
2. Define the business events security context rule using that function and specify the value of the `dt.security_context` attribute.
   The value of the `dt.security_context` attribute can be a literal value that you provide, or you can specify the name of another attribute of which the value will be used as the value of `dt.security_context`.

### Leverage existing tags at the source

You can define the security context at the source via [OneAgent](/docs/ingest-from/dynatrace-oneagent/oneagent-security-context "Learn how to set up Grail permissions for OneAgent."), [OpenTelemetry](/docs/ingest-from/opentelemetry/opentelemetry-security-context "Set up Grail permissions for OpenTelemetry."), or [Kubernetes labels or annotations](/docs/ingest-from/setup-on-k8s/guides/metadata-automation/k8s-metadata-telemetry-enrichment "Guides for telemetry enrichment on Kubernetes"). This allows you to use your existing labels and tags to facilitate permissions in Dynatrace.

## Create a business events security context rule via OpenPipeline

Recommended optional

Business event security context is based on rules that contain a matcher and the `dt.security_context` attribute definition, and they're set as a processor in OpenPipeline. The matcher narrows down the available business event records for executing this specific rule, while the value source type specifies the value of the `dt.security_context` attribute.

For more information, read about [OpenPipeline processors](/docs/platform/openpipeline/concepts/processing#processor "Learn the core concepts of Dynatrace OpenPipeline processing.").

1. Go to ![Settings](https://dt-cdn.net/images/settings-icon-256-38e1321b51.webp "Settings") **Settings** > **Process and contextualize** > **OpenPipeline** > **Business events**.
2. Go to the **Pipelines** tab and select an existing pipeline or create a new one with an existing dynamic route that would apply on the incoming business event.
3. Select the **Permission** tab to view business events processing security processors that are in effect on that pipeline, reorder the existing processors, and create new processors.
   Processors are executed in the order in which they're listed, from top to bottom. This order is critical because the first user-defined processor that matches is executed. A business events security processor consists of the following:

   * **Name**: The name of the processor.
   * **Matching condition**: A business events processing function that narrows down the available business events data for executing this specific processor.
   * **Security context value**

     + **Field name**: The value will be copied from the field.
     + **Static string**: A constant literal will be used as value.
     + **Static array**: An array literal will be used as value.
   * **Security context field value**: The value of the security context, depending on the selected type.
4. To add a new security context processor, go to the **Permission** tab and select  **Processor** > **Set security context** within the chosen pipeline.
5. Add a matching condition to your processor by pasting the business events processing function from your DQL query.
6. Choose the **Security context value** and provide the **Security context field value**:

   * If you selected **Field**, the value should be the name of the field used as a source to copy the value to the `dt.security_context` attribute.
   * If you selected **Literal**, the value should be the constant literal that will be used as the value in the `dt.security_context` attribute.

### OpenPipeline example

Your business events records contain attribute `geo.city.name` with a value of `Brussels`. Only users in the `team_EU` user group should have access to them and you want to create a business events security context rule that:

* Filters all records that contain `Brussels` as a value of the `geo.city.name` attribute.
* Adds a `dt.security_context` attribute with the `team_EU` value to all filtered business events records.

To create a new rule

1. Go to ![Notebooks](https://dt-cdn.net/images/notebooks-768-046137830a.webp "Notebooks") **Notebooks** and execute a DQL query using a [business events processing function](/docs/observe/business-observability/bo-event-processing/bo-events-processing-matcher "This is the DQL matcher in events in the classic pipeline ."):

   ```
   fetch bizevents



   | filter matchesValue(geo.city.name, "Brussels")
   ```
2. Copy the business events processing function of the DQL query: `matchesValue(geo.city.name, "Brussels")`.
3. Go to ![Settings](https://dt-cdn.net/images/settings-icon-256-38e1321b51.webp "Settings") **Settings** > **Process and contextualize** > **OpenPipeline** and navigate to the pipeline where you want to add the processor.
4. Select the **Permission** tab, and select  **Processor** > **Set security context**.
5. Define your new processor:

   * **Name**: `team_EU - Brussels`
   * **Matching condition**: `matchesValue(geo.city.name, "Brussels")`
   * **Security context value**: select **Static string**
   * **Security context field value**: `team_EU`
6. Select **Save**.

## Create a business events security context rule via the classic pipeline

optional

If you havenât upgraded to Grail and OpenPipeline yet, follow this section for processing via the classic pipeline.

Business event security context is based on rules that contain a matcher and the `dt.security_context` attribute definition.

* The matcher narrows down the available business events records for executing this specific rule.
* Value source type specifies the value of the `dt.security_context` attribute.

Go to **Settings** > **Business Observability** > **Security context** to view business events processing security rules that are in effect, reorder the existing rules, and create new rules. Rules are executed in the order in which they're listed, from top to bottom. This order is critical because the first user-defined rule that matches is executed.

Expand **Details** to examine a rule definition. A business events processing security rule consists of the following:

* **Rule name**: The name for the rule.
* **Matcher**: A business events processing function that narrows down the available business events data for executing this specific rule.
* **Select value source type**

  + **Field**: Value will be copied from field.
  + **Literal**: Constant literal will be used as value.
* **Value**: Value of the source type.

To add a business events security context rule:

1. Select **Add rule** on the **Business event security context** page.
2. Add a **Matcher** to your rule by pasting the [business events processing function](/docs/observe/business-observability/bo-event-processing/bo-events-processing-matcher "This is the DQL matcher in events in the classic pipeline .") from your DQL query.
3. Select value source type.
4. Provide the **Value** for the source type.

   * If you selected **Field**, the value should be the name of the field used as a source to copy the value to the `dt.security_context` attribute.
   * If you selected **Literal**, the value should be the constant literal that will be used as the value in the `dt.security_context` attribute.

### Classic processing example

Your business events records contain the attribute `geo.city.name` with the value `Brussels`. Only users in the `team_EU` user group should have access to them and you want to create a business events security context rule that:

* Filters all records that contain `Brussels` as a value of the `geo.city.name` attribute.
* Adds a `dt.security_context` attribute with the `team_EU` value to all filtered business events records.

1. Go to the **Logs and events** page and execute a DQL query using a [business events processing function](/docs/observe/business-observability/bo-event-processing/bo-events-processing-matcher "This is the DQL matcher in events in the classic pipeline ."):

   ```
   fetch bizevents



   | filter matchesValue(geo.city.name, "Brussels")
   ```
2. Copy the business events processing function of the DQL query: `matchesValue(geo.city.name, "Brussels")`.
3. Go to **Settings** > **Business Analytics** > **Security context** and select **Add rule** on the **Business event security context** page.
4. Define your new rule:

   * **Rule name**: `team_EU - Brussels`
   * **Matcher**: `matchesValue(geo.city.name, "Brussels")`
   * **Select value source type**: select **Literal**
   * **Value**: `team_EU`
5. Select **Save changes**.

## Check security context

optional

Once new business events data arrives, it's processed by business event security context rules or a processor (depending on the availability of OpenPipeline). A new `dt.security_context` attribute is added with a value of `team_EU`. You can go to ![Notebooks](https://dt-cdn.net/images/notebooks-768-046137830a.webp "Notebooks") **Notebooks** and check that new business events records are processed by your `team_EU - Brussels` business events security context rule. Use the DQL to view all business events records containing the `team_EU` value in the `dt.security_context` attribute:

```
fetch bizevents



| filter matchesValue(dt.security_context, "team_EU")
```

Based on this attribute, you can now create security-related user and group policies. See, [Permissions in Grail](/docs/platform/grail/organize-data/assign-permissions-in-grail "Find out how to assign permissions to buckets and tables in Grail.").

## Related topics

* [Dynatrace Query Language](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language.")
* [DQL matcher in business event in the classic pipeline](/docs/observe/business-observability/bo-event-processing/bo-events-processing-matcher "This is the DQL matcher in events in the classic pipeline .")


---


## Source: bo-event-processing.md


---
title: Business event processing
source: https://www.dynatrace.com/docs/observe/business-observability/bo-event-processing
scraped: 2026-02-17T21:27:41.349371
---

# Business event processing

# Business event processing

* Latest Dynatrace
* Explanation
* 4-min read
* Published Dec 11, 2025

Business events that you ingest from various sources can be processed before analysis.

We recommend utilizing OpenPipeline as a scalable, powerful solution to manage and process business events. If you don't have access to OpenPipeline, use the classic pipeline.

## OpenPipeline

OpenPipeline is the Dynatrace Platform solution for managing and processing data from various sources. It enables effortless data handling at any scale and format on the Dynatrace platform. It provides the following benefits:

* Contextual data transformation: OpenPipeline extracts data with context and transforms it into more efficient formats.
* Unified processing language: [Dynatrace Query Language (DQL)](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language.") is used as a processing language, offering one syntax for all Dynatrace features and more advanced options for processing.
* Pipeline concepts: You can split business event ingest traffic into different pipelines with dedicated processing, data and metric extraction, permissions, and storage.
* Additional processors: You can use additional processors, for example, to add or remove fields.
* Enhanced data extraction: Extract business events from logs with more data extraction options.
* Improved performance and higher throughput.

To get started, see [OpenPipeline](/docs/platform/openpipeline "Scale Dynatrace platform data handling with Dynatrace OpenPipeline.").

## Classic pipeline

Business event processing via the classic pipeline is the legacy solution to process business events in Dynatrace.

Even though the classic business events processing pipeline is still available for some environments, we recommend switching to business event processing with OpenPipeline. Business event processing with the classic pipeline will be deprecated at some point in the future.

To learn more, see [Business event processing via classic pipeline](/docs/observe/business-observability/bo-event-processing/bo-processing-classic-pipeline "Process business event data in Dynatrace via the classic pipeline.").

### Classic pipeline and OpenPipeline

If you created processing rules via the classic pipeline, we recommend that you manually migrate them to OpenPipeline.

To migrate your business event processing rules to OpenPipeline

1. In OpenPipeline, [create new custom pipelines and routes](/docs/platform/openpipeline/getting-started/tutorial-configure-processing "Configure ingest sources, routes, and processing for your data in OpenPipeline.").
2. In the classic pipeline, delete your processing rules.

If you don't migrate your existing rules, it's still possible to use OpenPipeline in combination with the classic pipeline. The processing flow looks as follows:

1. In OpenPipeline, [create new custom pipelines and routes](/docs/platform/openpipeline/getting-started/tutorial-configure-processing "Configure ingest sources, routes, and processing for your data in OpenPipeline.").
2. Data is processed by [custom pipelines](/docs/platform/openpipeline/concepts/processing#types "Learn the core concepts of Dynatrace OpenPipeline processing.").
3. If data doesn't match any route, it's routed by the default route to the [classic pipeline](/docs/platform/openpipeline/concepts/processing#types "Learn the core concepts of Dynatrace OpenPipeline processing.").
4. Business events are processed as defined by the classic pipeline.

## Use cases

* Parse data to the ideal format for your use cases.
* Extract metrics.
* Specify retention periods.
* Assign security context.

## Related topics

* [OpenPipeline processing examples](/docs/platform/openpipeline/use-cases/processing-examples "Explore scenarios of how to use OpenPipeline processing in Dynatrace powered by Grail.")
* [DQL matcher in OpenPipeline](/docs/platform/openpipeline/reference/dql-matcher-in-openpipeline "Examine specific DQL functions and logical operators for log processing.")


---


## Source: bo-events-capturing.md


---
title: Business event capture
source: https://www.dynatrace.com/docs/observe/business-observability/bo-events-capturing
scraped: 2026-02-17T21:21:45.201861
---

# Business event capture

# Business event capture

* Latest Dynatrace
* How-to guide
* 14-min read
* Updated on Oct 21, 2025

To get started with business events, you first need to define the scope of the data you want to capture. The approach you use depends on the source of the business events.

There are five sources for business events.

* [**OneAgent**](#report-business-event-oneagent)

  Configure in Dynatrace to add capture rules, triggers, data fields, and more.
* [**Web and mobile RUM**](#report-business-event-rum)

  Obtain RUM business events by leveraging a dedicated method of the RUM JavaScript API, OneAgent for Mobile, or OpenKit.
* **External sources**

  Configure external business or IT systems to send business events in JSON format to the [business events API (REST endpoint)](/docs/observe/business-observability/bo-api-ingest "Set up authentication for and ingest business events via API.").
* [**Logs**](#logs)

  Leverage logs as an additional source for business events via OpenPipeline.
* [**Workflows**](#workflows)

  Use the **Ingest business event** action within Workflows to generate business events from automated tasks.

## Get business events via OneAgent

OneAgent version 1.253+

OneAgent Full-Stack Monitoring mode is mandatory for the hosts in which you want to capture business events.

To capture business events using OneAgent, you need to first enable the feature.

1. Go to **Settings** > **Preferences** > **OneAgent features**.
2. Enable the OneAgent business events feature for the technologies appropriate for your environment.

   You need to restart the application process before you can capture business events from the process.

Configuration requires one or more capture rules that consist of **triggers**, **mandatory data fields**, and **optional event data fields**.

The table below contains examples of mandatory (`event.type`, `event.provider`) and optional (`event.category`) data fields.

| Field | Type | Description | Examples |
| --- | --- | --- | --- |
| `event.category` | string | Standard categorization based on the significance of an event according to the [ITIL event management standardï»¿](https://en.wikipedia.org/wiki/Event_management_(ITIL)) | `Availability` |
| `event.type` | string | The unique type identifier of a given event | `buy-asset`, `sell-asset`, `login` |
| `event.provider` | string | Source of the event, for example, the name of the component or system that generated the event | `OneAgent`, `easyTrade.com`, `easyTravel.com` |

### Supported technologies

Supported technologies for data extraction from HTTP requests are listed in the tables below.

#### Data extraction from incoming HTTP requests

Agent

OneAgent feature

Min. version

Technologies supported

Min. version

Compressed body

Full body

Min. version

Compressed body

Full body

**Enablement (event capture)**

**Enablement (event capture)**

**Enablement (event capture)**

**application/json (payload capture)**

**application/json (payload capture)**

**application/json (payload capture)**

**Header: XML (payload capture)**[1](#fn-1-1-def)

**Header: XML (payload capture)**[1](#fn-1-1-def)

**Header: XML (payload capture)**[1](#fn-1-1-def)

Webserver

Webserver Business Events

1.253

* Apache ![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")
* NGINX ![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")
* IIS ![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

1.253

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

1.275

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

.NET

.NET Business Events

1.253

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

1.253

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

1.279

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

Java

Java Business Events (incoming HTTP)

1.253

Servlet ![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

* Tomcat[2](#fn-1-2-def)
* Jetty
* JBoss
* Wildfly
* Websphere Liberty
* WebLogic
* Undertow
* GlassFish
* Netty[3](#fn-1-3-def)

1.253

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

1.275

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

Node.js

Node.js Business Events

1.259

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

1.259

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

n/a

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

Golang

Go Business Events

1.263

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

1.263

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable") (1.265)

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

n/a

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

1

Supported content types for XML capture on Java are `application/xml`.

2

Tomcat 10 supportâOneAgent version 1.263+ and **Java Servlet 5.0** Oneagent feature (in **Settings** > **Preferences** > **OneAgent features**) required

3

Supported from OneAgent version 1.323+. FileRegions are not supported.

#### Data extraction from outgoing HTTP requests

Agent

OneAgent feature

Min. version

Technologies supported

Min. version

Compressed body

Full body

Min. version

Compressed body

Full body

**Enablement (event capture)**

**Enablement (event capture)**

**Enablement (event capture)**

**application/json (payload capture)**

**application/json (payload capture)**

**application/json (payload capture)**

**Header: XML (payload capture)**[1](#fn-2-1-def)

**Header: XML (payload capture)**[1](#fn-2-1-def)

**Header: XML (payload capture)**[1](#fn-2-1-def)

Webserver

n/a

n/a

n/a

n/a

n/a

n/a

n/a

n/a

n/a

.NET

n/a

n/a

n/a

n/a

n/a

n/a

n/a

n/a

n/a

Java

Java Business Events (outgoing HTTP)

1.297+

HTTP Clients

* Apache HTTP Client 4.x
* OK HTTP Client 3.4+

1.297+

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

1.297+

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

Node.js

n/a

n/a

n/a

n/a

n/a

n/a

n/a

n/a

n/a

Golang

n/a

n/a

n/a

n/a

n/a

n/a

n/a

n/a

n/a

### Capturing variants

This feature is available for OneAgent version 1.309+ on Webserver, .NET and Java. For all other technologies, apply [the technology support table](#supported-technologies).

Define how incoming business events should be parsed based on their content-type.

To access the default configuration,  **Search** for `Capturing variants` and select it.

Content-type

Parser

\*/json

JSON

\*/xml

XML

application/x-www-form-urlencoded

URL encoded

text/plain

Text

Those settings apply on the global level and are therefore used for all deployed agents on supported technologies.

### Configure business event sources on OneAgent

OneAgent version 1.253+

To configure business event sources on OneAgent

1. Go to **Settings** > **Business Observability** > **OneAgent Business Event Sources**.
2. Go to **Incoming** or **Outgoing** tab.
3. Select **Add new capture rule** and name your rule.
4. Select **Add trigger** to define a condition that will trigger a business event.

   Determine the data source for your trigger, such as the request **Body**, **Path**, or **HTTP Header**. Then select an **Operator** and define a **Value**. This allows you to match content retrieved from the source to the value you define. When OneAgent matches the trigger, a business event is generated.

   * The **Summary** displays the entire trigger rule (for example, `Request - Path starts with '/api/trade/BuyAssets`).
   * By default, the **Value** is not case sensitive. Turn on **Case sensitive** if you want your trigger to consider the case of the source.
   * Triggers are connected by `AND` logic within a rule; if you set multiple triggers, all of them need to be fulfilled to capture a business event.
   * The first matching rule is executed per OneAgent per request.
   * We recommend that you set specific triggers. If a trigger is too general and results in multiple identical rule matches, you will get multiple business events. For example, if you have a trigger condition `contains api`, and the term `api` is used in many of your applications, data can end up being captured from where it shouldn't be.

   Do not create triggers where the value is simply `/`; this can lead to overloading and shutting down your application environment.
5. Select the **Event provider** source and value.

   This describes the source of the event, such as `www.easytrade.com`. The data source for this field can be a fixed value that you provide or it can be extracted from the event.
6. Select the **Event type** source and value.

   This describes the type of event sent by the event provider, such as `Asset purchase`.
7. Optional Select **Event category** source and value to add helpful context to the event (for example, add a stock exchange name such as `NASDAQ`).

   This step concludes the configuration of a business event that will be generated each time the trigger criteria are matched. This might be sufficient if all you need is to count the number of matching events (for example, to answer the question of how many asset purchases were made). In most cases, however, you will want to add event attributes for more granular insight (described in the step below). Attributes are data fields extracted from the event JSON or XML payload.
8. Select **Add data field** in **Event data**. Provide a field name, and then provide the data source and value from the JSON or XML payload. This describes the attribute-value pair that will be associated with the event, such as `accountId`, `amount`, `instrumentId`, or `price`. Adding such pairs can help you answer more complex questions such as how many accounts purchased a particular asset, which assets are purchased most often, or which accounts make the largest asset purchases.

   Example buy-asset request JSON file

   ```
   {



   "accountId":6,



   "amount":10,



   "instrumentId":1,



   "price":157.025



   }
   ```
9. Select **Save changes**.
10. Ensure that the rule is set to **Enabled**.

OneAgent capture rules can also be defined at the host or host-group level to limit the scope of capture.

### Example of data extraction from JSON payloads

The following table shows additional examples of how to extract data from JSON payloads.

Example request payload JSON file for OneAgent

* Request URLâ`example.dynatrace.com/api?action=addItems`
* Request headers

  + `Accept`â`*/*`
  + `Accept-Encoding`â`gzip, deflate`
  + `Accept-Language`â`en-US,en;q=0.9`
  + `Connection`â`keep-alive`
  + `Content-Length`â`64`
  + `Content-Type`â`application/json`
* Request payload

  ```
  {



  "time":"2022-03-12T12:16:36.5881611+00:00",



  "transactionId":"1748-2b59-5c78-9c75-f500-274a-88f5-7965",



  "user":{



  "user.id":"1684588",



  "userName":"johndoe",



  "name":"John",



  "surname":"Doe",



  "email":"me@johndoe.one"



  },



  "order":{



  "order.id":"58449798",



  "retailer":{



  "id":"558",



  "name":"HappyShop"



  },



  "amount":240.44,



  "currency":"usd",



  "tags":[



  "fancy",



  "modern",



  "classic",



  "vintage"



  ],



  "items":[



  {



  "itemId":"674",



  "price":175.99,



  "productName":"Product A",



  "productCategory":"Furniture",



  "quantity":1



  },



  {



  "itemId":48,



  "price":12.89,



  "productName":"Product Z",



  "productCategory":"Decoration",



  "quantity":5



  }



  ]



  }



  }
  ```

| Field name | Source | Path | Result | Description |
| --- | --- | --- | --- | --- |
| transactionId | Request â Body | transactionId | 1748-2b59-5c78-9c75-f500-274a-88f5-7965 | Captures a top-level attribute. |
| userName | Request â Body | user.userName | johndoe | Captures a nested attribute. |
| priceOfItems | Request â Body | items.0.price | 175.99 | Captures the first array item attribute. |
| Last tag | Request â Body | order.tags.-1 | Vintage | Captures the last element of an array. |
| Second-last tag | Request â Body | order.tags.-2 | Classic | Captures the second-last element of an array. |
| FullBody | Request â Body | \* | The entire body as a string | Captures the entire request body. |

### Examples of data extraction from XML payloads

The extraction of data from XML payloads works with the same syntax as for JSON.

#### Example 1: Extracting attributes and values from basic XML

```
<LogoutRequest id=â102030AFâ>



<accountId>100</accountId>



</LogoutRequest>
```

| Field name | Source | Path | Result | Description |
| --- | --- | --- | --- | --- |
| LogoutRequest id | Request â Body | LogoutRequest.@id | 102030AF | Captures XML tag attribute. |
| accountId | Request â Body | LogoutRequest.accountId | 100 | Captures XML tag value. |

#### Example 2: Extracting attributes and values from XML with namespaces

```
<?xml version="1.0" encoding="UTF-8"?>



<s:Envelope xmlns:s="http://schemas.xmlsoap.org/soap/envelope/">



<s:Body>



<Tagh1 xmlns="http://tempuri.org/">



<Tagh2 cusType="A" xmlns:a="http://schemas.datacontract.org/ex" xmlns:i="http://www.w3.org/2001/XMLSchema-instance">



<a:Code i:nil="true" />



<a:Name>MyCustomer1</a:Name>



<a:TotalValue>45</a:TotalValue>



</Tagh2>



</Tagh1>



</s:Body>



</s:Envelope>
```

| Field name | Source | Path | Result | Description |
| --- | --- | --- | --- | --- |
| CustomerName | Request â Body | s:Envelope.s:Body.Tagh1.Tagh2.a:Name | MyCustomer1 | Captures XML tag value. |
| CustomerType | Request â Body | s:Envelope.s:Body.Tagh1.Tagh2.@cusType | A | Captures XML tag attribute value. |

### Examples of data capture from requests and responses

Data capture from requests and responses is available for both JSON and XML.

The following table shows examples of how to extract data from requests and is based on the [JSON payload example above](#json).

| Field name | Source | Path | Result | Description |
| --- | --- | --- | --- | --- |
| ContentType | Request â HTTP Header | Content-Type | application/json | Captures a certain request header. |
| Action | Request â Query String parameters | action | addItems | Captures a certain query string parameter. |

Example of header wildcard capture

* Field nameâ`RequestHeaders`
* SourceâRequest â HTTP header
* Pathâ`*`
* DescriptionâCapture all header attributes
* ResultâAll header attributes as a string, separated by spaces

Example of query string wildcard capture

* Field nameâ`QueryParameters`
* SourceâRequest â Query string parameters
* Pathâ`*`
* DescriptionâCapture all query string parameters
* ResultâAll query string parameters as a JSON-compatible string

## Get business events from RUM

Business events are available for all Dynatrace RUM technologies (web RUM, mobile RUM, and OpenKit). RUM business events can be obtained by leveraging a dedicated method of the RUM JavaScript, OneAgent for Mobile, or OpenKit.

Check the sections below for instructions on how to report business events for different platforms.

[#### RUM JavaScript](/docs/observe/business-observability/bo-events-capturing#send-business-event-rum-js "Capture business events for Dynatrace Business Observability.")[#### Android](/docs/observe/digital-experience/mobile-applications/instrument-android-app/instrumentation-via-oneagent-sdk/oneagent-sdk-for-android#report-business-event "Learn how to enrich mobile user experience monitoring in Android using OneAgent SDK.")[#### iOS](/docs/observe/digital-experience/mobile-applications/instrument-ios-app/customization/oneagent-sdk-for-ios#report-business-event "Enrich mobile user experience monitoring using OneAgent SDK for iOS.")

#### Cordova[1](#fn-2-1-def)

[#### Xamarin](/docs/observe/digital-experience/mobile-applications/cross-platform-frameworks/xamarin-nuget#report-business-event "Monitor Xamarin apps with Dynatrace OneAgent.")[#### Flutter](https://pub.dev/packages/dynatrace_flutter_plugin#businessevent)[![.NET MAUI](https://dt-cdn.net/images/dotnetmaui-aea483621e.svg ".NET MAUI")

#### .NET MAUI](/docs/observe/digital-experience/mobile-applications/cross-platform-frameworks/maui#report-business-event "Monitor .NET MAUI applications with Dynatrace OneAgent.")[#### React Native](https://www.npmjs.com/package/@dynatrace/react-native-plugin#business-event-capturing)[#### OpenKit](/docs/ingest-from/extend-dynatrace/openkit/dynatrace-openkit-api-methods#report-business-event "Read how Dynatrace OpenKit can be used from the developer's point of view.")

1

To report business events for the native part of Cordova applications, follow the instructions for **Android** or **iOS**. For the web part, use the **RUM JavaScript**.

Send business events via RUM JavaScript

```
let attributes = {



"event.name": "Confirmed Booking",



"page": "booking-confirmation",



"product": "Danube Anna Hotel",



"amount": 358.35,



"currency": "USD",



"reviewScore": 4.8,



"arrivalDate": "2022-11-05",



"departureDate": "2022-11-15",



"journeyDuration": 10,



"adultTravelers": 2,



"childrenTravelers": 0



};



dynatrace.sendBizEvent('com.easytravel.funnel.booking-finished', attributes);
```

Business events are only captured for monitored sessions. When the RUM JavaScript is disabled either through a special method or due to [cost and traffic control](/docs/observe/digital-experience/web-applications/additional-configuration/configure-cost-and-traffic-control-web "Leverage the cost and traffic control setting in Dynatrace to reduce session usage for web applications."), business events are not reported for such sessions. Note that this behavior might be subject to change in the future, potentially allowing business events to be sent to Dynatrace regardless of session monitoring.

## Log to Business events

Use OpenPipeline to convert incoming logs to business events. This is useful if logs contain business-relevant information or no other ingest path for business events is available.

See the following log example to get started:

```
{



"content": "{\"user\": \"009494\", \"ordervalue\": 1000}"



}
```

1. Go to ![Settings](https://dt-cdn.net/images/settings-icon-256-38e1321b51.webp "Settings") **Settings** > **Process and contextualize** > **OpenPipeline** > **Logs** > **Pipelines**.
2. You can have multiple pipelines. Select the one that is used to take your loglines into processing.
3. Go to the **Data extraction** tab.
4. Open the  **Processor** list on the left and select **Business event** to add a new Business Event processor.
5. Fill out the fields with the following data:

   * **Name**: `OrderBizEventFromLog`
   * **Matching condition**: `matchesPhrase(content,"ordervalue")`
   * Define **Event type** as **Static string** and enter: `biz.fromlog.order`
   * Define **Event provider** as **Static string** and enter: `customlog`
   * Set **Field extraction** to the fields that should be extracted: `Extract all fields`
6. Select **Save**.

![Log to Business Events Extraction](https://dt-cdn.net/images/logs-extraction-3020-925a020b55.png)

## Generate business events from Workflows

You can generate business events from workflows to capture and persist business-grade data. Examples of how this can be used include:

* Daily ingestion of currency conversion rates for use in cost calculations.
* Ingestion of third-party information daily or on a scheduled basis to capture business data from a system not monitored by Dynatrace.
* Ingestion of aggregated business information for baselining, forecasting, and anomaly detection.

You can generate business events as a workflow action by selecting the **Ingest business event** action type.

![Adding an Ingest business event step to a workflow](https://dt-cdn.net/images/workflow-ingestoption-1920-813b4f1271.png)

The **Ingest business event** action can take data either from user-provided JSON data, or by executing DQL. For example:

* Select JSON to ingest JSON data from an API call response in a previous Workflow step, or to provide a custom-defined block of data.
* Select DQL to query grail for fields to store in the business event. This enables the querying and aggregation of business-quality metrics from other sources of data in Grail.

In addition to the **Business event data** source, **Ingest from** attribute, default values for **Event provider**, **Event type**, and **Event category** attributes should be provided to generate the business event. These values might be overridden by the data provided in the JSON or DQL statement. For example, if your DQL query returns a value for `event.type`, this value will be written to the business event instead of the default value.

If default values aren't provided, `event.provider` and `event.type` will be assigned attribute value `Unknown` when the event is generated, while `event.category` will be assigned attribute value `Other`.

![Workflow showing ingest business event step with manually entered JSON data](https://dt-cdn.net/images/workflow-ingest-json-1920-61c801b99e.png)

The **Business event data** can include one or more events to be generated. This applies to both DQL and JSON options:

* A DQL query that returns multiple rows of data will generate as many business events.
* JSON data with multiple elements will generate as many business events. For example, the following JSON payload generates three separate currency-related events:

  ```
  [



  {



  "currency.code":"USD",



  "currency.name":"US dollar",



  "exchange.rate":"1.1718"



  },



  {



  "currency.code":"EUR",



  "currency.name":"Euro",



  "exchange.rate":"1.0000"



  },



  {



  "currency.code":"GBP",



  "currency.name":"Pound sterling",



  "exchange.rate":"0.86320"



  }



  ]
  ```

  You can use the JSON payload mentioned earlier by parameterizing **Business event data (JSON)** input as follows:

  ```
  {{ result("stepname")| to_json }}
  ```

  You need use the `| to_json` instruction to convert the previous step response into correctly-formatted JSON. This will include all payload content, including JSON, body, and headers. To omit the body and headers, and ingest only the JSON content, use the following:

  ```
  {{ result("stepname")["json"]}}
  ```

After the successful execution of the workflow and the **Ingest business event** action, business events will be generated based on your parameters. The following is an example of a successful execution:

![Workflow generated business events](https://dt-cdn.net/images/basic-events-query-1411-5a2f06f0a5.png)

Use of DQL

Workflow steps have a limited execution time window. We recommend using a separate **Execute DQL Query** workflow step for long-running complex DQL queries. Then, you can use the returned data in the **Ingest business event** using the parameterization support (`{{ result(âstepnameâ }}`).

This action requires the `storage:events:write` permission defined in the Workflow authorization settings. For more information, see [User permissions for workflows](/docs/analyze-explore-automate/workflows/security "Guide on security aspects of workflow automation in Dynatrace Workflows")

If you're using DQL, your workflow actor needs permissions to run the DQL statement against necessary buckets.

### Handling JSON response data

Some use cases require you to use JSON data returned by an API call from a previous workflow action, and convert it into correct JSON that can be used with **Ingest business event**. For example, let's say you want to ingest currency rates from an external service on a daily basis and persist them as business events that can be used in other queries. To do that, you can define a workflow that triggers two steps:

1. Issues an HTTP request to any API.
2. Ingests a business event.

![Workflow showing ingest business event step with JSON command](https://dt-cdn.net/images/json-example-workflow-1920-0b05a61493.png)

The HTTP request returns the JSON payload content in the following format:

```
{



"success": true,



"timestamp": 1754946555,



"base": "EUR",



"date": "2025-08-11",



"rates": {



"USD": 1.161388,



"JPY": 171.953923,



"AUD": 1.78907,



"CAD": 1.600567



}



}
```

For our use case, we want to keep only the rates data.

Next, you need to configure the **Ingest business event** action. To do that

1. Choose the **Ingest from** > **JSON** option.
2. Define **Event provider (default)**, **Event type (default)**, and **Event category (default)**.
3. Using the **Business event data (JSON)** editor, define the data to be sent as the following:

   ```
   {"rates": {{result("step1")["json"]["rates"]| to_json }}}
   ```

   To make it easier to edit the syntax, you can select  **Maximize** to maximize the editor. You can also select  **Preview** to preview the results of the action execution.

   This will ensure that only the rates section from the JSON payload are kept in the generated business event.

   ![Editing JSON input in editor](https://dt-cdn.net/images/json-example-editor-1920-c5bc59b926.png)

After the workflow is executed, you can query the business event and get the following data:

![Raw business event data with JSON](https://dt-cdn.net/images/json-example-rawdata-1364-d2b4ff01f2.png)

To extract individual rates from the event, use the following DQL:

```
fetch bizevents



| filter event.type == "currency"



| parse rates, "JSON:rates"



| fields aud = rates[AUD], usd=rates[USD]
```

Run in Playground

![Using DQL to extract currency rate from array](https://dt-cdn.net/images/json-example-extractrate-1418-38638e4a57.png)

## Related topics

* [Simplify access to critical business data with OpenPipeline.ï»¿](https://www.dynatrace.com/news/blog/openpipeline-simplify-access-to-critical-business-data/)


---


## Source: bo-events-enrichment.md


---
title: Business event enrichment
source: https://www.dynatrace.com/docs/observe/business-observability/bo-events-enrichment
scraped: 2026-02-17T05:10:41.413109
---

# Business event enrichment

# Business event enrichment

* Latest Dynatrace
* Reference
* 10-min read
* Updated on Mar 19, 2024

Dynatrace SaaS version 1.253+

Dynatrace automatically enriches business events with additional context to enhance analysis and facilitate drill-down navigation. Specifically, Dynatrace adds certain properties to your business events, for example, information on geolocation, operating system, application, and more. You can then use [Dynatrace Query Language (DQL)](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language.") to analyze your business events.

## OneAgent and external business event enrichment

Fields defined in the table below can be used to enrich business events. Some fields are automatically added depending on their source (OneAgent or API).

Key

Added via

Description

Example

Data type

`timestamp`[1](#fn-1-1-def)

OneAgent

Time when the event originated, typically when it was created by the source[2](#fn-1-2-def)

`11/21/2022 8:53:58 AM`

timestamp

`time`[1](#fn-1-1-def)

API

Time when the event originated, typically when it was created by the source[2](#fn-1-2-def)

`11/21/2022 8:53:58 AM`

timestamp

`dt.entity.host`

OneAgent

Host where OneAgent captured the event

`HOST-2F700D78236C40D6`

string

`dt.entity.process_group_instance`

OneAgent

Process group instance where OneAgent captured the event

`PROCESS_GROUP_INSTANCE-6E5F1839CADBEC98`

string

`event.id`[3](#fn-1-3-def)

OneAgent

API

Unique identifier of the event autogenerated by Dynatrace (for OneAgent)

`iejRl3Li8kN_7uIekdUvQ`

string

`event.kind`

OneAgent

API

High-level kind of the event; the field is always set to `BIZ_EVENT` regardless of the ingested value.

`BIZ_EVENT`

string

`event.provider`[1](#fn-1-1-def)

OneAgent

API

Source of the event, for example, the name of the component or system that generated the event; the default value is `Unknown`.

`www.easytrade.com`

string

`event.type`[1](#fn-1-1-def)

OneAgent

API

Event type identifier; the default value is `Unknown`.

`com.easytrade.get-instruments`

string

`event.category`

OneAgent

API

Event category; the default value is `Other`.

`Other`

string

`span_id`

OneAgent

ID of the request as known by the caller; autogenerated by Dynatrace

`c1a6d280f407fba9`

string

`trace_id`

OneAgent

ID of the whole trace forest that is used to uniquely identify a distributed trace through a system; autogenerated by Dynatrace

`c21f35f1c5a639eef472961f0513ead8`

string

`trace_sampled`

OneAgent

Indicates if the caller has recorded the trace data; autogenerated by Dynatrace

`TRUE`

bool

`traceparent`

OneAgent

Field that identifies the incoming request in a tracing system; autogenerated by Dynatrace

`00-c21f35f1c5a639eef472961f0513ead8-c1a6d280f407fba9-01`

string

1

Not an enriched field per se, but required by the Dynatrace internal schema

2

If no original timestamp is available, it's populated at ingest time.

3

Set by OneAgent and required by CloudEvents specification, but not enforced by the Dynatrace internal schema

## RUM business event enrichment

Business events [reported via RUM](/docs/observe/business-observability/bo-events-capturing#report-business-event-rum "Capture business events for Dynatrace Business Observability.") are enriched with fields relevant to the device and user, such as geolocation, device, browser, and application. The following fields are added automatically, depending on the application typeâweb or mobile.

### Top-level fields

The top-level fields contain generally relevant information for all events and data points.

Key

Description

Example

Data type

Override possible

Required or optional

Available for

`timestamp`

Time when the event originated, typically when it was created by the source[1](#fn-2-1-def)

`1649822520123123123`

timestamp[2](#fn-2-2-def)

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

req

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

1

If no original timestamp is available, it's populated at ingest time.

2

Unix timestamp in nanoseconds

### Event fields

The `event` namespace contains common identification, categorization, and context on events in Dynatrace.

Key

Description

Example

Data type

Override possible

Required or optional

Available for

`event.type`[1](#fn-3-1-def)

Type identifier of the event

`com.easytravel.funnel.booking-finished`

string

N/A

req

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`event.id`

Unique identifier of the event

`5547782627070661074_1647601320000`

string

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

req

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`event.kind`

High-level kind of the event

`BIZ_EVENT`

string

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`event.provider`

Source of the event, for example, the name of the component or system that generated the event[2](#fn-3-2-def)

`www.easytravel.com`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

1

Not enriched automatically; you set it via API when you report a business event.

2

Full domain name for web, app bundle for mobile, and application ID for OpenKit

### Dynatrace RUM fields

The `dt.rum` namespace contains Dynatrace RUM-specific fields.

Key

Description

Example

Data type

Override possible

Required or optional

Available for

`dt.rum.schema_version`

Version of the event

`1.0`

string

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

req

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`dt.rum.application.id`

ID of the Dynatrace RUM application

`ea7c4b59f27d43eb`

string

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

req

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`dt.rum.instance.id`

ID of the RUM application instance[1](#fn-4-1-def)

`3735928559`

string

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

req[2](#fn-4-2-def)

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`dt.rum.session.id`

ID of the user session

`HOPCPWKILUKHFHWRRQGBHHPAFLUJUOSH-0`

string

![Not applicable](https://dt-cdn.net/images/icon-red-cross-1f1142a5dc.svg "Not applicable")

req[2](#fn-4-2-def)

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

1

Also known as the internal user ID or visitor ID

2

If available

### Device fields

The `device` namespace contains information on the device running an application.

Key

Description

Example

Data type

Override possible

Required or optional

Available for

`device.manufacturer`

Name of the device manufacturer

`Apple`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Not available for web applications](https://dt-cdn.net/images/web-red-500-bc0398e4c1.svg "Not available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`device.model.identifier`

Model identifier of the device

`iPhone3,4`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Not available for web applications](https://dt-cdn.net/images/web-red-500-bc0398e4c1.svg "Not available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`device.is_rooted`

Indicates if a rooted device was detected.

`true`

bool

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Not available for web applications](https://dt-cdn.net/images/web-red-500-bc0398e4c1.svg "Not available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

### OS fields

The `os` namespace contains information on the operating system running an application.

Key

Description

Example

Data type

Override possible

Required or optional

Available for

`os.name`

Operating system name

`iOS`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`os.version`

Operating system version

`15.3.1`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Not available for web applications](https://dt-cdn.net/images/web-red-500-bc0398e4c1.svg "Not available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

### Browser fields

The `browser` namespace contains information on the browser running an application.

Key

Description

Example

Data type

Override possible

Required or optional

Available for

`browser.name`

Browser name

`Mozilla`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Not available for mobile applications](https://dt-cdn.net/images/mobile-red-500-0d57960e2c.svg "Not available for mobile applications")

`browser.version`

Detected browser version

`5.0`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Not available for mobile applications](https://dt-cdn.net/images/mobile-red-500-0d57960e2c.svg "Not available for mobile applications")

`browser.type`

Type of browser[1](#fn-5-1-def)

`desktop`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Not available for mobile applications](https://dt-cdn.net/images/mobile-red-500-0d57960e2c.svg "Not available for mobile applications")

`browser.user_agent`

Full user agent string as provided by the browser

`Mozilla/5.0 (Windows NT 10.0; Win64; x64)`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Not available for mobile applications](https://dt-cdn.net/images/mobile-red-500-0d57960e2c.svg "Not available for mobile applications")

1

Possible values: `desktop`, `mobile`, `tablet`, `robot`, `other`

### Client fields

The `client` namespace contains information on the initiator of a network connection.

Key

Description

Example

Data type

Override possible

Required or optional

Available for

`client.ip`

IP address of the client (IPv4 or IPv6)

`126.32.196.104`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

### Application fields

The `app` namespace contains information on the application sending the business event.

Key

Description

Example

Data type

Override possible

Required or optional

Available for

`app.version`

Full machine-readable version of the application

`5.23.15789`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Not available for web applications](https://dt-cdn.net/images/web-red-500-bc0398e4c1.svg "Not available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`app.short_version`

Short human-readable version of the application

`5.23`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Not available for web applications](https://dt-cdn.net/images/web-red-500-bc0398e4c1.svg "Not available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`app.bundle`

Name of the bundle or package

`com.example.easytravel`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Not available for web applications](https://dt-cdn.net/images/web-red-500-bc0398e4c1.svg "Not available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

### Geolocation fields

The `geo` namespace contains geolocation information.

Key

Description

Example

Data type

Override possible

Required or optional

Available for

`geo.continent.name`

Full continent name

`North America`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`geo.country.name`

Full country name

`Canada`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`geo.region.name`

Full region name

`Quebec`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")

`geo.city.name`

Full city name

`Montreal`

string

![Applicable](https://dt-cdn.net/images/icon-green-check-700-c9ea81e533.svg "Applicable")

opt

![Available for web applications](https://dt-cdn.net/images/web-green-500-ec9418f975.svg "Available for web applications") ![Available for mobile applications](https://dt-cdn.net/images/mobile-green-500-9473b3c1eb.svg "Available for mobile applications")


---


## Source: business-flow.md


---
title: Business Flow
source: https://www.dynatrace.com/docs/observe/business-observability/business-flow
scraped: 2026-02-17T21:21:43.742123
---

# Business Flow

# Business Flow

* Latest Dynatrace
* App
* 5-min read
* Updated on Dec 05, 2025

About the app

IT teams and business owners can use ![Business Flow](https://dt-cdn.net/images/business-flow-480-362159ca2c.png "Business Flow") **Business Flow** to monitor and analyze critical business process flows. You can track end-to-end process delays, detect process anomalies, and report business key performance indicators (KPIs).

![Business Flow](https://dt-cdn.net/images/business-flow-480-362159ca2c.png "Business Flow") **Business Flow** helps with diagnosing process issues and prioritizing process optimization opportunities to improve business outcomes.

Prerequisites

Required

* Define and activate business events for each process step.
* Identify a unique identifier (correlation ID) that is common to all process steps (for example, `order_id`). The name of the correlation ID may differ between steps; the value is used to connect steps into a single flow.

Recommended

* Identify business events that indicate incidents or business exceptions, such as credit card payment errors, product outages, shipping exceptions, or other non-IT process issues.
* Identify a business KPI of your choice, such as revenue, that can be extracted from a business event attribute.

## Before you begin

* A business process is structured into steps or milestones, which are interconnected through a directional flow and branching paths.
* Each step or milestone can include business events that indicate the progression of the business flow to that specific point. Additionally, these steps can encompass business events that describe errors or exceptions impacting the business process.
* Business events, whether for step definitions or exceptions, are configured based on their `event.provider` and `event.type` values. To utilize these events within ![Business Flow](https://dt-cdn.net/images/business-flow-480-362159ca2c.png "Business Flow") **Business Flow**, the captured business events must align with these definitions.

  Example:

  Consider a business event in EasyTrade that captures the creation of a credit card order. To use this event as part of a milestone definition, such as **Create credit card order**, it is essential to define a specific `event.type` and `event.provider` for that business event. This allows the event to be selected for the **Create credit card order** step.

  Similarly, for a business exception, the `event.type` and `event.provider` must be unique to that business event, enabling its use in a step as a business exception definition.

### Permissions

The following table describes the required permissions.

Permission

Description

app-engine:functions:run

Run Business Flow calculations

storage:bizevents:read

Read business events from GRAIL

storage:buckets:read

Read system data from GRAIL

state:app-states:read

Read and share configurations in app-states

state:app-states:write

Write configurations in app-states

state:app-states:delete

Delete configurations in app-states

app-settings:objects:read

Read configurations in app-settings

app-settings:objects:write

Write configurations in app-settings

settings:objects:read

Read settings objects

settings:objects:write

Write settings objects

10

rows per page

Page

1

of 1

## Installation

Make sure the app is [installed in your environment](/docs/manage/hub#install "See the information about Dynatrace Hub.").

Get started

Concepts

Use cases

IT teams and business owners can use ![Business Flow](https://dt-cdn.net/images/business-flow-480-362159ca2c.png "Business Flow") **Business Flow** to monitor and analyze critical business process flows. You can track end-to-end process delays, detect process anomalies, and report business key performance indicators (KPIs).

![Business Flow](https://dt-cdn.net/images/business-flow-480-362159ca2c.png "Business Flow") **Business Flow** helps with diagnosing process issues and prioritizing process optimization opportunities to improve business outcomes.

![The Business Flow dashboard provides a real-time view into process and business KPI health.
Errors and business exceptions are displayed for the selected step.](https://cdn.hub.central.dynatrace.com/hub/bizflow_1.26.1_mainwindow.png)![View your business flow KPIs over time and define alerts based on anomaly detection analyzers](https://cdn.hub.central.dynatrace.com/hub/bizflow_1.26.1_kpis.png)

1 of 2The Business Flow dashboard provides a real-time view into process and business KPI health.
Errors and business exceptions are displayed for the selected step.

## Overview

The **Overview** tab serves as the landing page, where you can start discovering ![Business Flow](https://dt-cdn.net/images/business-flow-480-362159ca2c.png "Business Flow") **Business Flow** and explore a demo business flow configuration or the last one accessed in a single view.

From here, you can:

* Analyze your configured business flows. It contains all the user interface elements for analyzing your critical business processes.
* Explore the entire tree of your processes, planning and zooming in the central main area, select steps to understand the flows, and search for specific groups of flows based on filters in the right panel.
* Drill down into the top business KPIs to see them evolving over time and create alerts based on anomaly detection.

## Business flows

Use the business flows sidebar on the left to choose which configuration you want to explore.

From here, you can:

* Find business flows classified as entities and recently modified.
  Select  to duplicate or remove any existing business flow configuration.
* Collapse that sidebar to get more screen space for navigation on the tree view.

## Filter correlation IDs

On the landing page in the right panel, you can use several filtering options to focus on the matching business flows.

* Search for specific values of the correlation IDs.
* Filter by a step to find all the business flows that may have reached or are in transit/dropped to that step and by the status of the business flow.

## Investigate problems

The Problem mode enables precise analysis of any IT problem that may affect a business process.

* This mode highlights the most relevant metrics associated with the alert, focused on the appropriate timeframe.
* Additionally, it offers quick access to the underlying problem, allowing you to efficiently diagnose and resolve issues.

## Learning modules

Go through the following processes to learn using ![Business Flow](https://dt-cdn.net/images/business-flow-480-362159ca2c.png "Business Flow") **Business Flow**:

[01Set up Business Flow

* How-to guide
* Follow the instructions on how to successfully set up Business Flow.](/docs/observe/business-observability/business-flow/set-up-business-flow)[02Business Flow Details

* Explanation
* Explore Business Flow Details.](/docs/observe/business-observability/business-flow/business-flow-details)[03Business Flow KPIs

* Explanation
* Discover how key performance indicators can help you track performance and identify improvement opportunities within the Business Flow app.](/docs/observe/business-observability/business-flow/reported-kpis)

## Capabilities

With ![Business Flow](https://dt-cdn.net/images/business-flow-480-362159ca2c.png "Business Flow") **Business Flow**, you can

* Report business process KPIs, including flows completed (conversions), average flow completion time, business exceptions, and a business KPI of your choice.
* Visualize and analyze individual process flows from start to finish.
* Detect and explore uncompleted or dropped process flows to determine the cause, such as an IT error, a business exception, or abnormal inter-step transit time.
* Visualize process errors and business exceptions at each step.
* Define alerting on key metrics based on ![Anomaly Detection - new](https://dt-cdn.net/images/davis-anomalydetection-256-105da91594.png "Anomaly Detection - new") **Anomaly Detection** for short duration processes.

## Limitations

* Business processes can include no more than five branches in one step.
* Each step in a business process can be defined using no more than five different business events.
* A business process can have up to 20 nodes in total. A node is defined as a step or a branch; if a step has three branches, that counts as 3 nodes.
* Each step must have at least one active business event assigned, otherwise the configuration cannot be saved.
* The difference between timestamps on events that are in different steps must be at least one millisecond, as lower resolutions on durations are not supported, or may generate false positive alerts of disordered flows during the analysis of the business process.
* Alerting on key metrics is only possible when the average duration of a business process monitored is below 30 minutes, as ![Anomaly Detection - new](https://dt-cdn.net/images/davis-anomalydetection-256-105da91594.png "Anomaly Detection - new") **Anomaly Detection** only analyzes a 60-minute rolling window on defined metrics. See [Sliding window](/docs/dynatrace-intelligence/anomaly-detection/anomaly-detection-configuration#sliding-window "How to set up an alert for missing measurements.") for more details.
* All business KPIs in ![Business Flow](https://dt-cdn.net/images/business-flow-480-362159ca2c.png "Business Flow") **Business Flow** are the result of running complex DQL queries thatâdepending on the amount of data scannedâcan hit the default read data limit. If you reach this limit in any of the queries, you'll be notified and offered the option to run the DQL queries limitless on that configuration (once or by default).

  The limitless mode on a configuration can be switched off in the edit mode.

  See [DQL best practices](/docs/platform/grail/dynatrace-query-language/dql-best-practices "Best practices for using Dynatrace Query Language."), to learn more about data limits.

[![Hub](https://dt-cdn.net/images/hub-512-82db3c583e.png "Hub")

### Explore in Dynatrace Hub

Track your business performance indicators and results, detect problematic processes, and prioritize opportunities.](https://www.dynatrace.com/hub/detail/business-flow/?internal_source=doc&internal_medium=link&internal_campaign=cross)

## Related topics

* [[Blog] Business Flow: Why IT operations teams should monitor business processesï»¿](https://www.dynatrace.com/news/blog/business-flow-why-it-operations-teams-should-monitor-business-processes/)


---


## Source: business-process-monitoring.md


---
title: Business process monitoring
source: https://www.dynatrace.com/docs/observe/business-observability/business-process-monitoring
scraped: 2026-02-16T21:31:45.028451
---

# Business process monitoring

# Business process monitoring

* Latest Dynatrace
* Tutorial
* 3-min read
* Published Feb 19, 2025

Business processes are the automation backbone of modern businesses, and they must operate efficiently to meet business goals. Most business processes can impact customer experience, either positively or negatively. From procurement to order fulfillment, from customer onboarding to service request tracking, most organizations rely on hundreds, if not thousands, of business processes. These business processes depend on your IT systems to achieve their business goals efficiently and at scale.

## Introduction

Use Business Flow to monitor and optimize business processes. Gain real-time visibility into key performance indicators and detailed analytics to improve customer satisfaction, increase efficiency, and reduce cost.

With Business Flow, you can:

* Define the sequence of important process steps or process milestones, including branches and loops.
* Define business exceptions to report at each step.
* Analyze individual end-to-end process flows.
* Detect dropped or stalled process flows.
* Track process KPIs, including end-to-end process timing and inter-step delays

## Target audience

This article is intended for business analysts and process managers who understand how their businesses are performing in real time and look for optimal execution of their business processes. You should have a basic knowledge of how business events are captured and some domain knowledge of the business area you are trying to analyze.

## Prerequisites

Required

* Business events for each process step must be defined and active.
* Identify a unique identifier (correlation ID) that is common to all process steps (for example, `order\_id`). The name of the correlation ID may differ between steps; the value is used to connect steps into a single flow.

Recommended

* Identify business events that indicate incidents or business exceptions, such as credit card payment errors, product outages, shipping exceptions, or other non-IT process issues.
* Identify a business KPI of your choice (such as revenue) that can be extracted from a business event attribute.

## Use cases

### Track and report process KPIs

Business Flow reports three standard KPIs: Average duration, errors, and conversions. You can also add a business KPI of your choice.

In many cases, process duration is one of the most important indicators of process health. Many process optimization initiatives focus on reducing delays. In Business Flow, the average duration KPI represents end-to-end elapsed time. For each step, the average duration to the subsequent step(s) is also reported. These values help to focus improvement efforts on process "hot spots."

See a screenshot of Business Flow Details.

![Track and report process KPI](https://dt-cdn.net/images/bizflow-track-and-report-process-kpis-1-1920-2393a669bd.png)

See a screenshot of KPIs over time.

![Track and report process KPIs over time](https://dt-cdn.net/images/bizflow-track-and-report-process-kpis-2-1920-c383a1ab52.png)

### Analyze an individual process flow

You can analyze any process flow in detail.

For in-progress flows at any step, select the step box in the main tree panel. From the right-hand panel, choose **Explore**, then select the step of interest.

![Explore individual process flow](https://dt-cdn.net/images/bizflow-analyze-an-individual-process-flow-1-1920-fdfe34db1b.png)

You can open the resulting table in a Notebook for deeper exploratory analytics.

![Analyze an individual process flow](https://dt-cdn.net/images/bizflow-analyze-an-individual-process-flow-2-1920-c106d3b3fa.png)

### Analyze dropped or stalled process flows

When a process step takes longer than normal to reach a subsequent step, it is flagged as a drop. It may be stuck permanently, or it may simply be experiencing an unusually long delay.

For a list of the dropped flows at any step, select the step box in the main tree panel. In the right panel, you can filter to only show dropped.

![Explore dropped or stalled process flows](https://dt-cdn.net/images/bizflow-analyze-dropped-or-stalled-process-flows-1-1920-465ff57c95.png)

### Analyze business exceptions

For business exceptions, select a step box in the main tree panel that shows business exceptions; from the right-hand panel, filter by Business exceptions, then select the unique flow of interest.

![Explore Business exceptions ](https://dt-cdn.net/images/bizflow-analyze-business-exceptions-1-1920-b9d0d38fba.png)

![Analyze Business exceptions on detail](https://dt-cdn.net/images/bizflow-analyze-business-exceptions-2-1920-970495e9c7.png)

## Conclusion

Hereâs a brief list of some common business processes. Your organization will have a large portfolio of business processes, most of which are likely unmonitored. See a sampling list of such processes below.

* Customer support process
* Order processing
* Trade settlement
* Customer or employee onboarding or offboarding
* Procurement process
* Payment process
* IT service requests
* Loan approval
* Insurance claims processing
* Patient discharge
* Citizen requests for public services
* Free trial activation
* Quality assurance
* Subscription service fulfillment


---


## Source: business-reporting.md


---
title: Business reporting
source: https://www.dynatrace.com/docs/observe/business-observability/business-reporting
scraped: 2026-02-17T21:30:03.430363
---

# Business reporting

# Business reporting

* Latest Dynatrace
* Tutorial
* 3-min read
* Published Feb 19, 2025

Track business metrics, KPIs, and SLOs in real time, automatically in context with IT infrastructure and services to promote collaboration between business and IT teams. Prioritize IT investments and automate remediation based on measurable business impact.

## Introduction

Use business events to capture key business metrics and transform these into business KPIs. Business events are a special class of data well-suited for demanding business use cases. Gain real-time visibility into business health and enable exploratory analytics to optimize business outcomes and understand business anomalies.

Business events can come from OneAgent, RUM, log files, and external systems via an API. Dynatrace automatically enriches business events with Smartscape topology context, connecting business metrics to their supporting IT systems. OneAgent can capture business events from in-flight application payloads without requiring code changes, prioritizing business data to ensure the lossless precision many business use cases demand.

## Target audience

This article is intended for business analysts and process managers who understand how their businesses are performing in real time and look for optimal execution of their business processes. You should have a basic knowledge of how business events are captured and some domain knowledge of the business area you are trying to analyze.

## Use cases

### Define the metrics and dimensions you want to report

Often, business KPIs are a function of multiple data points. For example, to report a supplier lead time KPI, you would subtract the order request date from the order delivery date. The dates are ingested as business events and then transformed by a processing rule into a lead time KPI. Youâll want to include relevant dimensions such as supplier name and product to add important granularity to your reports. These dimensions are derived from fields within the business event.

### Determine where to capture the business events

OneAgent makes it easy to capture business events through simple configuration steps, without any custom code. If your application can be instrumented with OneAgent, this should be your preferred option.

Log files are a commonly used source, especially if your application already writes relevant business data to log files.

You can ingest business events from external systems through an API.

You can use Dynatrace real user monitoring (RUM) to capture business events from user interactions.

### Define processing rules

Processing rules help you filter, parse, and transform business data. You can add new calculated fields, mask or drop sensitive data, define retention policies through bucket assignment, and much more.

Build and share your [dashboard](/docs/analyze-explore-automate/dashboards-and-notebooks/dashboards-new "Create interactive, customizable views to visualize, analyze, and share your observability data in real time.").

Once stored in Grail, you can query your business event data interactively and analyze it using [DQL](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language."). DQL is the starting point for analysis, whether you use Notebooks, Dashboards, or the DQL Query API. You can use query results interactively or pin them to a dashboard as charts, tiles, or tables.

### Measure and optimize the business value of aggregator sales channels

Capture business events from aggregator APIs to extract business KPIs such as quotes sent, bookings, and average revenue. Segment these business KPIs into relevant cohorts or dimensions; examples include partner, product, and geography. Add relevant IT metrics such as API response time and API errors.

### Recover lost business by automating customer outreach

Track critical errors near the bottom of your conversion funnel that frequently cause users to abandon their journey. Segment users by potential or real customer value; cart amount, purchase history, or loyalty status. Create a Workflow to automate an appropriate outreach response such as an incentive to return or credit towards a future purchase.

### Reduce call center costs by alerting on business exceptions

Extract business error messages such as out-of-stock, credit declined, unknown product SKU, invalid invoice number, etc. from in-flight application payload. Define alerting thresholds and profiles to notify IT and business teams as appropriate.

### Track long-term seasonal trends

To report on trends, capture the relevant business metrics as business events. Ensure your business event bucket assignments have the retention characteristics needed to meet your analysis and reporting goals.


---


## Source: cost-and-carbon-optimization.md


---
title: Cost & Carbon Optimization
source: https://www.dynatrace.com/docs/observe/business-observability/cost-and-carbon-optimization
scraped: 2026-02-17T21:21:46.421229
---

# Cost & Carbon Optimization

# Cost & Carbon Optimization

* Latest Dynatrace
* App
* 8-min read
* Updated on Dec 21, 2025

About the app

* Calculates your IT carbon footprint at the data center, host, and Kubernetes infrastructure levels for multi hybrid-cloud infrastructure.
* Calculates your IT cost based on public price list from cloud vendors for each region and at host level.
* Translates utilization metricsâincluding CPU, memory, disk, and network I/Oâinto energy consumption in kWh and CO2 equivalents (CO2e).
* Reports energy consumption, carbon dioxide emissions, and list price costs in a single interfaceâwith drill-downs into physical hosts and Kubernetes infrastructure.
* Identifies opportunities to reduce costs and carbon emissions.

Prerequisites

### Permissions

The following table describes the required permissions.

Permission

Description

app-engine:apps:run

Run the Cost & Carbon Optimization app

state:app-states:delete

Delete workflow execution metadata

app-engine:functions:run

Run the Cost & Carbon Optimization data calculation and ingest

storage:events:write

Store Cost & Carbon Optimization events in GRAIL

storage:entities:read

Read entities from GRAIL

state:app-states:read

Read configuration and workflow metadata from app-states

state:app-states:write

Writes configuration and workflow metadata to app-states

automation:workflows:read

Read Cost & Carbon Optimization data ingest automation related data

automation:workflows:write

Creates/edits the Cost & Carbon Optimization data ingest automation

storage:bizevents:read

Read Cost & Carbon Optimization events from GRAIL

10

rows per page

Page

1

of 1

### Grant permissions to Workflows

Check if [Workflows](/docs/analyze-explore-automate/workflows "Automate IT processes with Dynatrace Workflowsâreact to events, schedule tasks, and connect services.") has the required permissions to run automations: open ![Workflows](https://dt-cdn.net/images/workflows-1024-b5708f3cf9.webp "Workflows") **Workflows**, select **Settings** > **Authorization settings** in the upper-right corner, and ensure that the following settings are enabled.

* `app-engine:apps:run`
* `app-engine:functions:run`
* `app-settings:objects:read`
* `app-settings:objects:write`
* `automation:workflows:read`
* `automation:workflows:write`
* `environment-api:entities:read`
* `iam:bindings:read`
* `state:app-states:read`
* `state:app-states:write`
* `state:app-states:delete`
* `storage:bizevents:read`
* `storage:buckets:read`
* `storage:entities:read`
* `storage:events:write`
* `storage:metrics:read`

For more information, see [Manage user permissions with IAM policies](/docs/manage/identity-access-management/permission-management/manage-user-permissions-policies "Working with policies") and [Workflow authorization settings](/docs/analyze-explore-automate/workflows/running "Run and monitor workflows created in Dynatrace Workflows.").

## Installation

Make sure the app is [installed in your environment](/docs/manage/hub#install "See the information about Dynatrace Hub.").

For carbon emissions calculation

Carbon dioxide emissions and energy consumption are calculated for hosts that are set up and monitored with OneAgent. Energy calculations are based on observed infrastructure metrics.

1. In ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization**, go to the **Hosts** tab.
2. Check that you have at least one host instrumented.

For public pricing costs calculations

* The data generation of carbon emissions must be active to allow generating public pricing cloud costs. You can activate it in the app's  settings.
* You need instrumented hosts with OneAgent and cloud vendor monitoring configured.
  The calculation of public price list costs requires the collection of metadata from a host as the cloud provider, cloud region, and the type of instance configured in the cloud vendor. This information is provided by configuring the cloud vendor monitoring for the hosts to be monitored with OneAgent. For more information, see [Ingest data](/docs/ingest-from "Learn how to install and configure ActiveGate and OneAgent on various platforms.").
* Enable **External requests** to the cloud vendors where the hosts are running to collect public price lists into Grail.

  External requests enable outbound network connections from your Dynatrace environment to external services. They allow you to control access to public endpoints from the AppEngine with app functions and functions in Dashboards, Notebooks, and Automations.

  1. Go to ![Settings](https://dt-cdn.net/images/settings-icon-256-38e1321b51.webp "Settings") **Settings** >  **General** > **External requests**.
  2. Select  **New host pattern**.
  3. Add the domain names.
  4. Select **Add**.

  This way you can granularly control the web services your functions can connect to.

  You need to add the following domain names

  + For AWS, add `*.amazonaws.com`
  + For Azure, add `azure.microsoft.com`
  + For Google Cloud, add `cloudbilling.googleapis.com`

Get started

Concepts

Use cases

Dynatrace ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** enables you to calculate and monitor your IT cost (public list price) and carbon footprint.

It tracks, reports, and helps you optimize the costs from your cloud infrastructure and reduce your carbon emissions resulting from your cloud and on-premises infrastructure electricity consumption.

## Landing page

The **Overview** tab provides an at-a-glance view of your IT carbon footprint.

![Cost & Carbon Optimization overview page summarizes carbon emissions, energy consumption, cloud costs, and optimization targets.](https://cdn.hub.central.dynatrace.com/hub/overview-page.png)

1 of 1Cost & Carbon Optimization overview page summarizes carbon emissions, energy consumption, cloud costs, and optimization targets.

* The **Carbon & Energy Footprint** summary reports total CO2e emissions for the selected and preceding timeframes for quick interval-based comparisons.
* Wasted energy summary is also reported with the selected and preceding timeframe.
* Publicly listed **cloud costs** report the sum of cloud costs for Dynatrace-monitored hosts for the user-selected timeframe, using publicly available list prices from each cloud provider. Costs are displayed for the selected and preceding timeframes.

* **Optimization recommendations** reports idle hosts and underutilized hosts with their cost and wasted energy measured.

  To adjust thresholds influencing these calculations to your needs, navigate to the upper-right corner and select  to open the app's settings.
* The **Accumulated carbon footprint over time** chart shows the accumulated carbon footprint and energy consumption over time.
* With the **Carbon versus business KPI** chart, you can [compare carbon emissions over time with a business key performance indicator (KPI) of your choice](#business-health), derived from any of your captured business events.
* If cloud costs are calculated, two more charts display **Accumulated cloud cost over time** and **Cloud cost versus business KPI** defined.
* Carbon/energy data is measured for all hosts monitored by Dynatrace OneAgent, or by Dynatrace Kubernetes monitoring.

## Hosts

This tab allows you to view energy, carbon emissions, costs, and utilization information for OneAgent-monitored hosts.

![Host page displays host-level carbon, energy, cloud costs and optimization status](https://dt-cdn.net/images/hosts-page-1920-f2f5c2e743.png)

* The table includes all hosts with carbon/cost information for the user-selected timeframe, and displays metadata, energy consumption, CO2 equivalent, and cloud costs (cloud hosts only).
* Use the information from this table to identify and optimize hosts with low utilization and high energy consumption, high carbon emissions, and high costs.
* The table allows you to search for specific hosts, filter by data center, or filter by optimization state (`Idling`, `Scaling`, `Normal`âthresholds are defined in the app's  settings).
* Expand a host to see if average CPU, memory and network traffic utilization for the user-selected timeframe.
* The page includes an intent that allows you to query the underlying data using DQL in a notebook or dashboard and combine the carbon and energy data with other information in Dynatrace. This can be useful to customize optimization information using DQL. Tabular information can be downloaded to a CSV file by selecting the download link.
* This view includes only hosts that are monitored by Dynatrace OneAgent. Kubernetes infrastructure is shown in the **Kubernetes** tab. Cloud hosts not monitored by Dynatrace OneAgent are not displayed here.

## Kubernetes

![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** works only for Kubernetes monitoring on Grail; classic Kubernetes monitoring is not supported.

This tab allows you to view energy, carbon emissions, and utilization information for Kubernetes infrastructure monitored by Dynatrace.

![Kubernetes clusters GA](https://dt-cdn.net/images/kubernetes-clusters-ga-1920-e487b9d40c.png)![Kubernetes Namespaces GA](https://dt-cdn.net/images/kubernetes-namespaces-ga-1920-43a911839b.png)

1 of 2

Use the **Kubernetes** view to explore the energy consumption and carbon emissions of Kubernetes clusters, namespaces, and nodes by selecting the relevant link at the top of the page. Each view provides you with:

* A textual summary of carbon emissions and energy consumption
* Charts of the carbon emissions and energy consumption over the user-selected timeframe.
* A table of carbon emissions, energy consumption, and the relevant metadata for each Kubernetes entity.

Clusters

Namespaces

Provides a breakdown of carbon emissions and energy consumption for each monitored cluster, with an average count of nodes, namespaces, and workloads seen during the user-selected timeframe.

You can filter the table by cluster deployment type (for example, EKS, AKS, GKE) and search for a specific cluster names.

See [Average vs. actual values](#average-actual-value) for additional information on the use of average and actual counts.

This table provides a breakdown of carbon emissions and energy consumption for each namespace, with an average count of pods and workloads seen during the user-selected timeframe.

You can filter the table by cluster name and search for specific namespaces.

See [Average vs. actual values](#average-actual-value) for additional information on the use of average and actual counts.

The **Clusters** and **Namespaces** views

* Provide contextual navigation to the Kubernetes entity in ![Kubernetes (new)](https://dt-cdn.net/images/kubernetes-new-1024-45d3de15d1.webp "Kubernetes (new)") **Kubernetes** by selecting the cluster, namespace, or node link in the table.
* List the average CPU and memory allocatable to help understand each entity's hardware resources.
* Include an intent that allows you to query the underlying data using DQL in a notebook or dashboard and combine the carbon and energy data with other information in Dynatrace. Tabular information can be downloaded to a CSV file by selecting the download link.
* The % metric change is also shown in each view, enabling you to quickly determine whether the scope or emissions of a Kubernetes entity have increased/decreased for the selected timeframe.

Use this information to

* Identify Kubernetes clusters and namespaces with the highest energy consumption and carbon emissions, and correlate this with Dynatrace ![Kubernetes (new)](https://dt-cdn.net/images/kubernetes-new-1024-45d3de15d1.webp "Kubernetes (new)") **Kubernetes** information on CPU and memory slack to identify the most costly and underutilized Kubernetes resources.
  Leverage this information with your application teams to reconsider optimizing their requests/limits to reduce slack and optimize carbon and costs.

  One-click navigation between ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** and ![Kubernetes (new)](https://dt-cdn.net/images/kubernetes-new-1024-45d3de15d1.webp "Kubernetes (new)") **Kubernetes** allows you to quickly drill down into more information about the Kubernetes resources to inspect resource requests and limits, quotas, or idle workloads/jobs that reserve resources but run infrequently.
* Track energy and carbon at the Kubernetes cluster and namespace over time and identify anomalies that need further investigation to understand what changed in the environment and whether the change was intentional and warranted. A large spike (increase) in energy and carbon could be a normal side effect of increased usage, but it could signal an issue or change with automatic scaling or resource provisioning.
  Examples: Changes to the type of node instances being used, or an unnecessary increase in associated storage.
* Measure and compare carbon emissions from single-use servers to Kubernetes infrastructure to ensure that migration efforts are meeting the desired sustainability goals.
  Organizations may even consider gamifying this and challenging their engineering teams to develop solutions that delight end-users and the environment.

### Average vs. actual values

Kubernetes is a dynamic environment, and the number of nodes and pods changes continuously.

When reporting on carbon and energy over longer timeframes, counts can be derived using the average count of nodes, pods, and other entities observed during the user-selected timeframe, or a more in-depth query can be made to the Kubernetes data in Grail to get actual distinct counts of each entity.

In a large Kubernetes environment, an actual count of distinct entities can take several seconds, whereas the average count typically takes milliseconds, and is sufficient for most use cases.

This selector may account for differences between ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** and ![Kubernetes (new)](https://dt-cdn.net/images/kubernetes-new-1024-45d3de15d1.webp "Kubernetes (new)") **Kubernetes**.

The query time for the actual data is dependent on the size of your Kubernetes infrastructure and the timeframe being analyzed.

* Using actual data over a 7-day period could take minutes, not seconds.
* Querying namespace actual data is significantly heavier than clusters or nodes, and we recommend using a short timeframe (1-2d) when using actual data within the **Namespaces** tab.

When using the deployment mode **Kubernetes platform monitoring + Application observability**, nodes and namespaces can be excluded from monitoring using the `namespaceSelector` (and `nodeSelector`).

As ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** only reports on the energy/carbon of namespaces/nodes that are monitored, energy and carbon emissions shown may be lower than expected when infrastructure is excluded from monitoring. Excluded namespaces will be displayed in the **Namespaces** table but will show no carbon/energy measurements.

In addition, internal workloads (such as `coredns`, `traefik`, and more) may be running that are not monitored by Dynatrace and energy/carbon data will not be included in the measurements.

## Learning modules

Go through the following process to learn using ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization**:

[01Set up Cost & Carbon Optimization

* How-to guide
* Install and set up Cost & Carbon Optimization.](/docs/observe/business-observability/cost-and-carbon-optimization/install-cost-and-carbon-optimization)[02Advanced analytics with Notebooks

* Reference
* Use Notebooks to analyze your carbon footprint data.](/docs/observe/business-observability/cost-and-carbon-optimization/advanced-analytics-with-notebooks)

### Methodology to estimate electricity consumption

Electricity consumption and carbon dioxide emission are calculated based on guidelines from the Sustainable Digital Infrastructure Alliance ([SDIAï»¿](https://dt-url.net/rga3uql)) with some contributions from the [Cloud Carbon Footprintï»¿](https://dt-url.net/0ic3u5z) project and internal research from Dynatrace. All of the methodology is aligned with the [GHG Protocolï»¿](https://dt-url.net/1re3uzm).

Energy is estimated based on the utilization metrics captured by Dynatrace OneAgent (in all modes for hosts) for CPU, memory, storage IO, and network. This estimation is performed every hour for each instrumented host through an automation.

The formula to estimate energy draw includes several parameters from external data sources and some assumptions.

The deviation from the measured power cannot be reliably determined due to the large variety of devices in the market and lack of statistical analysis.

### External data sources

* Random Access Memory energy usage data source

  + [How much power does memory use?ï»¿](https://dt-url.net/yt03uaw)
* CPU architecture sources (TDP, core, thread count) used in calculations

  + [Intel Product Specificationsï»¿](https://dt-url.net/ld23uvj)
  + [AMD Processor Specificationsï»¿](https://dt-url.net/nj83uae)
  + CPUs not officially listed are manually added through research, and the sources vary
* Storage energy usage

  + United States Data Center Energy Usage Reportâ[Energy Technologies Areaï»¿](https://dt-url.net/jl63u1r)
* Networking energy usage

  + Methodologyâ[Cloud Carbon Footprintï»¿](https://dt-url.net/cw83ura)

### Assumptions for electricity consumption

* CPU energy measurement:

  + Over a long period of time, a CPU running at 100% utilization will consume its TDP in power draw.
  + The base power draw of a CPU is 1/3 of its TDP
    It is known that true idle for CPUs is much lower, but CPU utilization hovers around a constant 10-20% for server applications, so true idle can never be reached.
  + One cloud vCPU equals one CPU thread
* Memory energy measurement:

  + Random Access Memory uses a constant of 3 W of power per 8 GB regardless of utilization.
* Storage energy measurement:

  + The power draw of a terabye of SSD storage is 1.2 W
  + All storage is assumed to be on an SSD
  + Storage power draw is constant
  + Cloud storage is replicated twice (or more)
* Network energy measurement:

  + 1 GB of networking (upload or download) consumes 1 W if sent outside of the local network.
  + 1 GB of networking (upload or download) consumes 0.12 W if remaining within a local network.

### Power Usage Effectiveness (PUE)

The final result of the formula includes the energy consumed by hardware and cooling. Cooling is included in a standard metric, Power Usage Effectiveness (PUEânot displayed directly in ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** but available in the raw data generated). This metric, used to measure the energy efficiency of a data center, is a ratio between the total power consumption of a data centerâincluding cooling, lighting, and support equipment energy useâand the power consumed by the IT infrastructure. It describes how much energy IT equipment uses compared to other electrical devices.

The Power Usage Effectiveness varies depending on a data center, but its predefined values for the main cloud providers are publicly available. The ones used in the calculation are:

* AWS and Azureâ[MethodologyâCloud Carbon Footprintï»¿](https://dt-url.net/cw83ura)
* Googleâ[EfficiencyâData Centersï»¿](https://dt-url.net/t5g3uwt)
* Rest of the worldâ[Global PUEsï»¿](https://dt-url.net/0ai3utu)

### Wasted energy calculation

Wasted Energy calculation is based on the assumption that every CPU cycle that is not utilized wastes about 1/3\*TDP (Thermal Design Power). The proportion of unutilized CPU cycles is roughly 100% - CPU usage %. With those assumptions, the Wasted Energy (WE) is calculated as WE = Wp \* 1/3 \* TDP, where Wp is 0 for 100% CPU utilization and 1 when the CPU utilization is equal to the idle threshold.

### Carbon emissions calculation

Carbon emissions are estimated in CO2 equivalent grams. The calculation is multiplying carbon intensity factors by total energy consumed calculated with previous methodology.

### Assumptions for carbon emissions

* A data centerâs carbon intensity is equal to the average carbon intensity of the country where it is located. This means that for large countries the national differences are not considered.
* Carbon intensity values used on calculations are an annual average.

### Carbon intensity data sources

* European Union (EU)â[Greenhouse gas emission intensity of electricity generationï»¿](https://dt-url.net/lm23uco)
* Rest of the worldâ[Carbon intensity of electricity generationï»¿](https://dt-url.net/1103ubu)

### Public price list costs calculation

![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** can collect public price list data from cloud providers available data on a daily basis, limited to the regions where OneAgents are deployed and cloud extensions are configured.

Every hour, the same workflow that generates the data for energy and emissions also calculates the hourly cost of a host based on the public price list matched with its cloud vendor, region, and instance type.

Both business events follow the Dynatrace Semantic Dictionary schema.

### Data centers and hosts

The **Data center emissions** table on the landing page shows costs, energy, and CO2e consumption per data center. Select a data center name to view details of its hosts.

The **Hosts** tab details cost, energy, and CO2e consumption per host. You can narrow your search using filters. For example, you can view underutilized hosts in a specific data center or top CO2e emitters.

Expand a hostname to see key infrastructure metrics: **CPU in use**, **Memory in use**, and **Receiving network traffic**. Select a host name to view the host details page in [![Infrastructure & Operations](https://dt-cdn.net/images/infrasctucture-operations-highresolution-1025-07d1bc45b5.png "Infrastructure & Operations") **Infrastructure & Operations**](/docs/observe/infrastructure-observability/infrastructure-and-operations "Monitor hosts, VMs, processes, and networks to detect issues and improve infrastructure performance.").

![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** is automatically connected to Dynatrace SmartscapeÂ® topology modeling, so it's easy to see the host details or use [![Notebooks](https://dt-cdn.net/images/notebooks-768-046137830a.webp "Notebooks") **Notebooks**](/docs/analyze-explore-automate/dashboards-and-notebooks/notebooks "Analyze, visualize, and share insights from your observability dataâall in one collaborative, customizable workspace.") for ad hoc analysis with DQL.

### Measure the energy of Dynatrace Kubernetes platform monitoring

Use ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** to measure energy and carbon emissions of your Kubernetes infrastructure.

![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** calculates the energy consumed (in Watt-hours) and carbon emissions (in grams of carbon dioxide emissions) of Kubernetes clusters and namespaces, and nodes, monitored by Dynatrace. All Dynatrace Kubernetes monitoring modes are supported, and data is calculated hourly and stored as [carbon Kubernetes events](/docs/semantic-dictionary/model/business-analytics#carbon-kubernetes-cluster-events "Get to know the Semantic Dictionary models related to Business Observability.").

Organizations using Kubernetes today, or looking to migrate workloads to Kubernetes, should consider using ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** in addition to the Dynatrace Kubernetes monitoring for the following reasons:

Energy use is a direct driver of cost
:   Cloud infrastructure costs are largely driven by energy consumption. CPU, memory, storage, and networking all consume power, and cloud providers factor this into pricing. By measuring energy usage, you're essentially tracking the underlying metric that influences cost.

Carbon footprint adds environmental context
:   Carbon footprint translates energy consumption into environmental impact. This is especially valuable for organizations with sustainability goals or ESG (Environmental, Social, and Governance) reporting requirements.

Optimization opportunities align
:   Inefficient clusters, namespaces, and nodes that consume more energy (and thus emit more carbon) are often the ones driving up cloud costs. Identifying high-carbon workloads can highlight areas for cost and environmental optimization.

The app will generate carbon Kubernetes bizevents in addition to regular carbon bizevents when the **Enable energy and emission data generation** option is enabled within the app's  settings.

You can report on Kubernetes infrastructure energy and carbon emissions using custom DQL and dashboards.

### Cluster calculations

Each Kubernetes node within the cluster is identified as a host, and ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** uses the host processor, and utilization information to calculate energy and carbon every hour using the **Cost & Carbon Optimization Calculation** workflow. The carbon and energy metrics of all nodes are summed and stored as a bizevent using: `type="carbon.measurement.k8s.cluster"`.

### Namespace calculations

Namespace carbon and energy values are derived by determining the % utilization of each namespace across the Kubernetes nodes it runs on, proportionally allocating energy and carbon from each node to the namespace. The carbon and energy metrics of all nodes are summed and stored as a bizevent using: `type="carbon.measurement.k8s.namespace"`.

Bear in mind the following:

* CPU processor information is only available when using **Kubernetes platform monitoring + Full-Stack observability**. Other monitoring modes provide only instance type. Dynatrace maintains a lookup table of cloud instance type to CPU and will fall back to a Dynatrace-derived Kubernetes median value of 240w TDP and 64 threads.
* For AWS Fargate, energy calculations assume a Graviton1 processor type (TDP 110w, 16 threads).

## Use cases

* Cost reporting and allocation
* Carbon reporting and allocation
* Carbon reduction
* Usage optimization
* Kubernetes infrastructure carbon optimization
* Cost anomaly management

## Troubleshooting

"An error occurred while loading the data. Please make sure you have the required user permissions."

This popup error message will list the permissions needed to use ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization**.

Ensure that a Dynatrace IAM administrator adds these policies to your user/group.

"Try adjusting the timeframe or columns"

* There's no data to display, potentially due to the timeframe selected, or due to carbon data not being generated correctly.
* If you enabled ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization** recently, please wait for up to 1-2 hours for the workflow to execute and start generating carbon data.
* You can check whether carbon or carbon Kubernetes data is being generated by running the following DQL in a notebook. Select the  icon on the page, right above any table in the **Hosts** or **Kubernetes** tabs, to **Open with** the underlying DQL in a notebook:

  ```
  fetch bizevents



  filter event.type == "carbon.measurement" or event.type == "carbon.measurement.k8s.cluster"
  ```

  Run in Playground
* Ensure that the Cost & Carbon Optimization workflow has the required authorization settings to function.

  Open ![Cost & Carbon Optimization](https://dt-cdn.net/images/cost-and-carbon-1024-4f5a603752.webp "Cost & Carbon Optimization") **Cost & Carbon Optimization**  settings page. If this is the problem, an error message with missing authorization settings will be displayed theres.
* Ensure that the Cost & Carbon Optimization workflow is scheduled to run every hour, that all steps within the workflow are enabled (not disabled), and that no errors are generated by the workflow.

  Review any workflow errors for further troubleshooting or contact our customer support.

"We encountered an unexpected issue. Please try again later."

* A platform issue is preventing the app from executing queries to report on carbon Kubernetes data.

  The issue may be temporary and will resolve itself shortly, or please reach out to our customer support.
* Kubernetes monitoring may not be set up or functioning correctly.

  Please check ![Kubernetes (new)](https://dt-cdn.net/images/kubernetes-new-1024-45d3de15d1.webp "Kubernetes (new)") **Kubernetes** to ensure that your Kubernetes infrastructure is being monitored correctly.

[![Hub](https://dt-cdn.net/images/hub-512-82db3c583e.png "Hub")

### Explore in Dynatrace Hub

Track and reduce your site infrastructure's carbon footprint.](https://www.dynatrace.com/hub/detail/carbon-impact/?internal_source=doc&internal_medium=link&internal_campaign=cross)

## Related topics

* [Analyze energy consumption and carbon emissions in hybrid cloud infrastructureï»¿](https://www.dynatrace.com/news/blog/analyze-energy-consumption-and-carbon-emissions-in-hybrid-cloud-infrastructure/)


---


## Source: end-to-end-example.md


---
title: Business events end-to-end example
source: https://www.dynatrace.com/docs/observe/business-observability/end-to-end-example
scraped: 2026-02-17T21:21:37.567621
---

# Business events end-to-end example

# Business events end-to-end example

* Latest Dynatrace
* Tutorial
* 7-min read
* Updated on Jan 28, 2026

This use case shows how your data can be captured, how a processing rule is set, and how this data can be further analyzed and visualized.

## Target audience

This article is intended for business analysts and process managers who understand how their businesses are performing in real time and look for optimal execution of their business processes. You should have a basic knowledge of how business events are captured and some domain knowledge of the business area you are trying to analyze.

## Scenario

EasyTrade is a simulated trading application with the following use cases:

* Money deposits and withdrawals
* Buy-assets and sell-assets transactions
* Basic dashboards for your trading activities
* Order a credit card

In this scenario, we suppose you need to calculate the total value of deposits placed on each account, the total and maximum dollar trading volume per account on EasyTrade, and the total value of buy-asset transactions per account in quick buy operations.

## Before you begin

You need to determine:

* The processing logic and rules
* The data fields needed

Because you need to calculate the dollar trading volume, your processing rule should multiply the price by the amount and add the result as a new field to the event:

**Dollar Trading Volume** = **Price** \* **Amount**

You also need to define the data items of the quick-buy-asset request. For example:

```
{



"accountId":6,



"amount":10,



"instrumentId":1,



"price":157.025



}
```

## Steps

1. Create a capture rule

1. Go to ![Settings](https://dt-cdn.net/images/settings-icon-256-38e1321b51.webp "Settings") **Settings** app.
2. Select **Collect and capture** > **Business Events** > **Incoming**.
3. Select **Add new capture rule** and set **Rule name** to `easyTrade- /v1/trade/buy`.
4. Select **Add trigger** and, in the **Summary** section, set the following:

   * **Data source**: `Request - Path`
   * **Operator**: `starts with`
   * **Value**: `- /v1/trade/buy`
5. Select again **Add trigger** and, in the **Summary** section, set the following:

   * **Data source**: `Request â HTTP Method`
   * **Operator**: `equals`
   * **Value**: `POST`
6. Define the **Event provider**:

   * **Data source**: `Fixed value`
   * **Fixed value**: `www.easytrade.com`
7. Define the **Event type**:

   * **Data source**: `Fixed value`
   * **Fixed value**: `com.easytrade.quick-buy`

     If you need to add more event types, select **Event type** separately for each event type. In this case, the event types will also be `com.easytrade.` and `com.easytrade.deposit`.
8. Define the **Event category:**

   * **Data source**: `Fixed value`
   * Optional **Fixed value:** you can leave it blank in this case.
9. In the **Event data** section, you need to add four data fields.

   For each data field:

   1. Select **Add data field**.
   2. Set the **Field name**, **Source**, and **Path** values according to the following table:

      Field name

      Source

      Path

      accountId

      Request-Body

      accountId

      amount

      Request-Body

      amount

      instrumentId

      Request-Body

      instrumentID

      price

      Request-Body

      price
10. Select **Save changes**.

2. Add fields

To create your data processing rule

1. Go to ![Settings](https://dt-cdn.net/images/settings-icon-256-38e1321b51.webp "Settings") **Settings** > **Process and contextualize** > **OpenPipeline**.
2. Select **Business Events** > **Pipelines** tab.
3. To create a new pipeline, select  **Pipeline** and enter the nameâ`Process EasyTrade data`.
4. In the **Processing** tab, select  **Processor** > **DQL** and name your processor `EasyTrade trade volume`.
5. Set your rule's **Matching condition** to the following [matcher-specific DQL query](/docs/platform/openpipeline/reference/dql-matcher-in-openpipeline "Examine specific DQL functions and logical operators for log processing."):

   ```
   matchesValue(event.provider, "www.easytrade.com")
   ```

   1. Details

   * If you needed to add only one event type (for example, `com.easytrade.buy-assets`), the matcher would be:

     ```
     matchesValue(event.type, "com.easytrade.buy-assets")
     ```
   * For two event types within the same event provider, the matcher would be:

     ```
     matchesValue(event.type, "com.easytrade.buy-assets") or matchesValue(event.type, "com.easytrade.sell-assets")
     ```
   * In this use case, however, you need to take all event types under the EasyTrade event provider, so it's sufficient to use:

     ```
     matchesValue(event.provider, "www.easytrade.com")
     ```
6. In the **DQL processor definition** field, add

   ```
   fieldsAdd trade_volume=amount*price
   ```
7. Select **Save**.
8. To create a new route, go to the **Dynamic routing** tab >  **Dynamic route** and specify:

   * A descriptive name for your new dynamic route
   * The **Matching condition** to be:

     ```
     matchesValue(event.provider, "www.easytrade.com")
     ```
9. Select the `Process EasyTrade data` pipeline, where the business events will be routed.
10. Select **Add**, then **Save**.
    Note: Any rule put under **Dynamic routing** in `Process EasyTrade data` pipeline will process data under the same matching condition.

3. Extract a business event metric

To add your business event metric

1. Go to ![Settings](https://dt-cdn.net/images/settings-icon-256-38e1321b51.webp "Settings") **Settings** > **Process and contextualize** > **OpenPipeline**.
2. Select **Business Events** > **Pipelines** tab.
3. Select the previously created `Process EasyTrade data` pipeline.
4. Go to the **Metric Extraction** tab.
5. Select  **Processor** > **Value metric** and name it `bizevents.EasyTrade.TradingVolume`.
6. Add **Matching condition** to your rule by pasting your [matcher-specific DQL query](/docs/platform/openpipeline/reference/dql-matcher-in-openpipeline "Examine specific DQL functions and logical operators for log processing."):

   ```
   matchesValue(event.type, "com.easyTrade.quick-buy")
   ```
7. Choose the **Field extraction** on which your metric will be basedâadd `trading_volume`.
8. In **Metric key**, name your metric `bizevents.EasyTrade.TradingVolume`.
9. You can also choose **Dimensions**, either **Pre-defined**, for example `dt.entity.host`, or **Custom**.
10. Select **Save**.

To display your metrics

1. Go to ![Notebooks](https://dt-cdn.net/images/notebooks-768-046137830a.webp "Notebooks") **Notebooks**.
2. Select  **Notebook** and name it `Notebook for business analytics metrics`.
3. To open the metrics query builder, add a new section and select  **Metrics**.
4. Find your metric in the search window, select **Run**, and display your results.

   You can also:

   * Pin the metric to a dashboard
   * Export your data to a CSV file
   * Share a link
   * Copy the DQL query
   * Use Dynatrace Intelligence to analyze the times series to detect anomalies or do a forecasting

4. Choose the retention period

If you need to store your data for one year, for example, for tax purposes, first you need to create a bucket for bizevents with a one year retention period.

1. Go to ![Settings](https://dt-cdn.net/images/settings-icon-256-38e1321b51.webp "Settings") **Settings** app.
2. Select  **Storage management** > **Bucket storage management**.
3. Select  **Bucket** and name the bucket with a unique identifierâ`easytrade-bizevents`.
4. Add the **Display name** `EasyTrade bizevents`.
5. Define the retention period for 365 days.
6. For **Bucket table type** choose `bizevents`.
7. Select **Create**.

After creating the bucket, you need to define the rules to store the incoming bizevents in that bucket.

1. Go to ![Settings](https://dt-cdn.net/images/settings-icon-256-38e1321b51.webp "Settings") **Settings** > **Process and contextualize** > **OpenPipeline**.
2. Select **Business Events** > **Pipelines** tab.
3. Select `Process Easytrade data` from the pipelines list.
4. Go to the **Storage** tab, select  **Processor** > **Bucket assignment**.
5. Name it `Easytrade data bucket assignment`.
6. Define **Matching condition** as `true` (since the routed bizevents all belong to the `EasyTrade data`).
7. Set **Storage** to `EasyTrade bizevents`.
8. Select **Save**.

5. Test your results

To see if your data was captured correctly

1. Select **Run in Playground**  below this query:

   ```
   fetch bizevents



   | filter event.provider == "www.easytrade.com"



   | filter event.type == "com.easytrade.quick-buy" OR event.type == "com.easytrade.deposit" OR event.type == "com.easytrade.quick-sell"



   | sort timestamp desc
   ```

   Run in Playground
2. Select **New notebook** and the query will run itself automatically.

   You should get results something like the following:

   ![Analyze your data with DQL](https://dt-cdn.net/images/analyze-your-data-with-dql-blurred-2634-444ada5143.png)

6. Analyze your data with DQL

At this point, you can start building your [DQL](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language.") queries to analyze the data loaded, define metrics, and create charts and dashboards.

You need to calculate:

* The total value of deposits per account in the last 30 days
* The total value of buy-asset transactions per account in the last 30 days
* The total and maximum dollar trading volume per account in the last 30 days

To analyze your data using DQL, select **Run in Playground**  below this query:

```
fetch bizevents, from:now()-30d, to:now()



| filter event.provider == "www.easytrade.com" and (event.type == "com.easytrade.quick-buy" OR event.type == "com.easytrade.deposit" OR event.type == "com.easytrade.quick-sell")



| fieldsAdd moneyTransfered = if(event.type == "com.easytrade.deposit", toDouble(amount)),



buyAssets = if(event.type == "com.easytrade.quick-buy", toDouble(amount)),



trading_volume= if(event.type == "com.easytrade.quick-sell",(amount*price))



| summarize sum(moneyTransfered), sum(buyAssets), sum(trading_volume), max(trading_volume), by:accountId
```

Run in Playground

Results table:

accountid

max(trading\_volume)

sum(buyAssets)

sum(moneyTransfered)

sum(trading\_volume)

6

19924.3187495

609491

1340160.809059129

526623.1340707905

7

47827.65870614

593648

1287203.0690401047

922284.0555854205

8

16718.08125

615123

1288892.9309932007

550042.8770441795

7. Display your results

You can display your results in several ways, including the following options.

* As a bar chart in ![Notebooks](https://dt-cdn.net/images/notebooks-768-046137830a.webp "Notebooks") **Notebooks**, limiting the result to the top 10 with maximum money transfer.

  1. Select  Options.
  2. In the right-side panel, select the **Bar** type of visualization.
* On a dashboard tile

  To pin your results to a dashboard in ![Notebooks](https://dt-cdn.net/images/notebooks-768-046137830a.webp "Notebooks") **Notebooks**

  1. Select  >  **Open with**.
  2. Select ![Dashboards](https://dt-cdn.net/images/dashboards-512-b1f1e9690b.png "Dashboards") **Dashboards**.
  3. Decide whether you want to open it with a new dashboard or an existing one.


---


## Source: explore-business-events.md


---
title: Explore Business Events
source: https://www.dynatrace.com/docs/observe/business-observability/explore-business-events
scraped: 2026-02-17T21:21:41.370416
---

# Explore Business Events

# Explore Business Events

* Latest Dynatrace
* App
* 2-min read
* Published Apr 25, 2023

About the app

![Explore Business Events](https://dt-cdn.net/images/biz-events-512-68c4be09db.png "Explore Business Events") **Explore Business Events** is a central repository of resources related to business events, including blog posts, documentation, direct product links and other applications relevant to business analysis, such as [Notebooks](/docs/analyze-explore-automate/dashboards-and-notebooks/notebooks "Analyze, visualize, and share insights from your observability dataâall in one collaborative, customizable workspace.") and [Dashboards](/docs/analyze-explore-automate/dashboards-and-notebooks/dashboards-new "Create interactive, customizable views to visualize, analyze, and share your observability data in real time."). It was designed to:

* Take you through the Dynatrace business analysis process end-to-end, starting from business event capture, through processing, and analyzing, to displaying final results on dashboards.
* Help you understand business analytics use cases and address your specific use case.

Prerequisites

### Permissions

The following table describes the required permissions.

Permission

Description

storage:bizevents:read

Is required to read bizevents from GRAIL

storage:buckets:read

Is required to read system data from GRAIL

settings:objects:read

Is required to read OneAgent rules

settings:objects:write

Is required to write OneAgent rules

app-settings:objects:read

Is required to read configurations in app-settings

storage:filter-segments:read

Read filter segments

storage:events:write

Required for workflow action

10

rows per page

Page

1

of 1

## Installation

Make sure the app is [installed in your environment](/docs/manage/hub#install "See the information about Dynatrace Hub.").

Get started

Concepts

Use cases

![Explore business events](https://dt-cdn.net/hub/Explore_business_events_2.png)

1 of 1Explore business events

## Process flow

The Business event process in Dynatrace consists of three main steps: Capture and Process, Analyze, and Create Dashboards. To see a use case covering all steps, go to [Business events use case](/docs/observe/business-observability/end-to-end-example "How to capture and process business events.").

### Capture and process

Displays all links relevant to business event capture and processing. To see a real-life scenario for capturing and processing business event data, go to [the capture section of the end-to-end use case](/docs/observe/business-observability/end-to-end-example#create-a-capture-rule "How to capture and process business events.").

### Analyze

Provides you with links to two places where you can run your DQL queries: Notebooks and the **Logs and event viewer**.

* To see examples of simple and advanced DQL queries for business analysis, go to [Analysis and examples](/docs/observe/business-observability/bo-analysis "Analyze and present business event data.").
* To see the analysis performed as part of the use case, go to [Use case analysis part](/docs/observe/business-observability/end-to-end-example#analyze-your-data-with-dql "How to capture and process business events.").

### Visualize

Provides you with links to places where dashboards can be created. To see an example dashboard created as part of the use case, go to [the end-to-end use case](/docs/observe/business-observability/end-to-end-example#display-your-results "How to capture and process business events.").

## Dynatrace resources for business events

* Blog posts supplement the documentation with real-life examples, use cases and how-to guides.
* Dynatrace live chat and the Dynatrace Community are interactive platforms that provide answers to your questions in real time and offer collaboration possibilities.
* Direct product links save your time and help you arrive exactly in the place a certain process step takes place, for example at the **Capture business events with OneAgent** product page.
* [Dynatrace Query Language (DQL)](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language.") is used to query and analyze your business events, in [Notebooks](/docs/analyze-explore-automate/dashboards-and-notebooks/notebooks "Analyze, visualize, and share insights from your observability dataâall in one collaborative, customizable workspace."), and the **Logs and event viewer**.
* Custom dashboards transform complex data from your queries into easy-to-follow visualizations.

[![Hub](https://dt-cdn.net/images/hub-512-82db3c583e.png "Hub")

### Explore in Dynatrace Hub

Understand the Dynatrace business analysis process end-to-end.](https://www.dynatrace.com/hub/detail/explore-business-events/?internal_source=doc&internal_medium=link&internal_campaign=cross)

## Related topics

* [View a blog post on business analytics.ï»¿](https://www.dynatrace.com/news/blog/dynatrace-enhances-business-observability/)
* [View the Dynatrace website devoted to business analytics.ï»¿](https://www.dynatrace.com/platform/business-observability/)
* [View the Dynatrace Query Language hub](/docs/platform/grail/dynatrace-query-language "How to use Dynatrace Query Language.")


---


## Source: microsoft-power-bi.md


---
title: Microsoft Power BI
source: https://www.dynatrace.com/docs/observe/business-observability/extensions/microsoft-power-bi
scraped: 2026-02-17T21:21:47.642299
---

# Microsoft Power BI

# Microsoft Power BI

* Latest Dynatrace
* Overview
* 2-min read
* Published Mar 17, 2025
* Preview

Preview

The Microsoft Power BI extension brings data from Grail directly into your Microsoft Power BI reports and dashboards by providing DQL queries.

The cost of executing DQL queries within Microsoft Power BI is equivalent to [the Dynatrace UI](/docs/license/subscription-and-license/subscription-and-license-dps "View license and subscription usage and consumption history Dynatrace Platform subscription licenses that were signed prior to April, 2023.").

## Limitations

* Import mode only
* Maximum result for DQL queries: 1,000,000 rows
* Maximum result for default tables (BizEvents, Events, Logs, Spans): 1,000 rows

## Setup

You need to have [Permissions in Grail](/docs/platform/grail/organize-data/assign-permissions-in-grail "Find out how to assign permissions to buckets and tables in Grail.") to execute queries and to view
certain tables in Grail.

Example for a policy statement to read Business Events:

```
ALLOW storage:buckets:read WHERE storage:table-name = "bizevents";



ALLOW storage:bizevents:read;
```

### Connect to Dynatrace Grail from Microsoft Power BI Desktop

To connect to a Dynatrace SaaS instance from Power Query Desktop, take the following steps.

1. Select **Dynatrace Grail DQL** in the **Get Data** experience. For more information, see [Where to get data | Power Queryï»¿](https://dt-url.net/x203wmh).
2. Enter your Dynatrace environment. This environment needs to be the latest Dynatrace version with Grail enabled. The DQL query is optional at this point, but should be used for more complex queries outside of the default built-in queries.
3. Sign into your environment to authenticate your permissions.
4. Enter your credentials into the pop-up window.
5. Once you authenticate, you'll see the message that you're signed in. Select **Connect**.
6. The **Navigator** page shows the record types available to select from Dynatrace Grail.
7. Select one of them, **Logs** for example, and run a fetch logs DQL query with a default 1,000 row limit. You require permissions on the Dynatrace environment in order to run this query successfully.
8. The **Advanced Editor** shows the M code the connector is using.
9. You can go back now to the optional DQL query from step 2 and type in a DQL query.
10. Import the results into Microsoft Power BI.

## Related topics

* [Microsoft Power BI extensionï»¿](https://dt-url.net/go03wp3)


---


## Source: salesforce-insights.md


---
title: Salesforce Insights
source: https://www.dynatrace.com/docs/observe/business-observability/extensions/salesforce-insights
scraped: 2026-02-17T21:21:36.137059
---

# Salesforce Insights

# Salesforce Insights

* Latest Dynatrace
* Extension
* 12-min read
* Updated on Oct 06, 2025

Salesforce Insights enables Salesforce administrators and IT operation teams to monitor their Salesforce environment.

## Setup

Learn how to set up Salesforce Insights and find out what kind of data it can capture.

Make sure to meet the following requirements to use the extension.

### Salesforce account

The Salesforce extension can capture four types of Salesforce data. Select each configuration for more details.

* The [Event Streaming](#event-streaming) configuration allows you to monitor the usage of your Salesforce CRM account.
* The [EventLogFile](#eventlog) configuration allows you to ingest log files from Salesforce into Dynatrace.
* The [API Queries](#soql) configuration allows you to ingest Salesforce Object Query Language (SOQL) data into Dynatrace. The data is ingested into Dynatrace as log events or Business Events.
* The [Platform Events](#platform-events) configuration allows you to subscribe to custom Salesforce Platform Events and ingest them as Business Events into Dynatrace.

### Authentication

Choose one of the three authentication methods that best suits your needs.

Connected App

User and Password

Client ID

In this mode the extension connects as a [connected appï»¿](https://dt-url.net/pv2c01v5). This is the recommended authentication mechanism.  
Technically, the app implements the [OAuth 2.0 JWT Bearer Flowï»¿](https://dt-url.net/yzs3qlb).

Dynatrace will ask for

* The **Consumer Key** of the connected app.
* The **Private Key** of the connected app.
* The **Subject** of the JWT tokenâthis is the username of a user that is part of a Profile in the connected app.

#### Requirements

* `openssl`âfor generating the certificate, not needed if you already have a certificate and a private key

  Note: on Windows, if you have `git` installed, you should also have a copy of `openssl`.  
  You should find it in a directory such as `C:\Program Files\Git\mingw64\bin` depending on your installation.

#### Certificate

The connected app must have a certificate that we later authenticate with Salesforce using this certificate's private key.

This command generates a certificate and a private key:

```
openssl req -newkey rsa:2048 -nodes -keyout key.pem -x509 -days 365 -out cert.pem
```

* The `cert.pem` file will be later added to the connected app.
* The `key.pem` file will be used by Dynatrace to authenticate with Salesforce.

#### Create the connected app

In Salesforce Lightning

1. Under **Setup** > **Apps** > **External Client Apps** > **Settings** make sure that `Allow creation of connected apps` is **On**.
2. Select **New Connected App**.

Name the app and add the contact email. Under **API (Enable OAuth Settings)**:

1. Check **Enable OAuth Settings**
2. If the Callback URL is not used, you can put in `http://localhost`
3. Check **Use digital signatures**
4. Upload the `cert.pem` file generated above under `Use digital signatures`.
5. In **Selected OAuth Scopes**, add these scopes:

   * **Manage use data via APIs (API)**
   * **Perform requests at any time (refresh\_token, offline\_access)**
6. Leave all other settings as default and select **Save**.

#### OAuth policy

1. Set up the **OAuth Policy** permitted users.
2. Under **Apps** > **App Manager**, Find the connected app and select **Manage**.
3. Click the **Edit Policies** button.
4. Under **OAuth Policies** select **Admin approved users are pre-authorized**.
5. Select **Save**.

#### Approved users

Identify users that can use the app. You can do it by adding Profiles to the **Application Profile Assignment** list.

1. On the connected app page, go to **Profiles** > **Manage Profiles**.
2. Add profiles that can use the connected app.
   Note: Later, any username from these profiles can be used as the **Subject** when configuring the extension.

Profiles need the following permissions for event streaming:

* General user permissions

  + `View Real-Time Event Monitoring Data`
* Administrative permissions

  + `Customize Application`
  + `View All Data`

To check the needed permissions documentation, see [how to Enable Access to Real-Time Event Monitoringï»¿](https://dt-url.net/5343qhf).

1. A regular Salesforce user with the permissions for the desired configuration (Event Streaming, Event Log File or SOQL).
2. The password for the user.
3. The [security tokenï»¿](https://dt-url.net/oz23qoo) for the user.

Note that this option is not recommended as user passwords and security tokens can change, it is usually used just to test the extension.

For [Client ID authenticationï»¿](https://help.salesforce.com/s/articleView?id=xcloud.remoteaccess_oauth_client_credentials_flow.htm&type=5), select **Enable Client Credentials Flow** in the Connected App or the External Client App settings, and provide the **Consumer Key** and **Consumer Secret**.

You will also need to select a user to run the app as. Do this under **App Manager** > **Manage** > **Edit Policies** > **Client Credentials Flow** > **Run As**.

Note that this is not as secure as a Connected App with a certificate.

### Enable extension

To enable the Salesforce Extension, you must have an Environment ActiveGate.

1. Find the extension in Hub and activate it.
2. Select **Add monitoring configuration** on the Configuration page.
3. Select an ActiveGate group.

   * One of these servers will need to access the salesforce API, which is publicly available.
   * The login URLs are: `https://login.salesforce.com` or `https://test.salesforce.com`
   * The Pub/Sub endpoints are: `api.pubsub.salesforce.com:7443` or `api.deu.pubsub.salesforce.com:7443`
   * A proxy can be configured later if necessary.

The parameters for the monitoring configuration

| Parameter | Description |
| --- | --- |
| **Endpoint name** | Choose a helpful name to identify the endpoint |
| **Login URL** | Choose from [Productionï»¿](https://dt-url.net/lui3q3b) or [Sandboxï»¿](https://dt-url.net/okk3qle) |
| **Pub/Sub URL** | Choose from **Global Endpoint** or **Europe (Frankfurt) Endpoint** |
| **Reporting Mode** | **Business Events**, **Logs**, **OpenKit (RUM)** |
| **Custom Application ID** | The Dynatrace Custom Application ID for the app you created before (only needed if a managed environment is used and OpenKit is used as reporting mode.) |
| **Authentication type** | Choose from [username and password](#user-password-authentication) or [connected app](#connected-app-authentication) |
| **Events filtering** | Choose which real-time events you'd like to send to Dynatrace, by default all are enabled |
| **Usernames Blocklist** | Optional list of Usernames to ignore; use this to block automation/API users from reporting |
| **Proxy** | Optional proxy; in case the ActiveGate cannot connect to the Salesforce URLs |

OpenKit

If you use OpenKit, we recommend starting with the Custom Application setup.

## Salesforce Data Ingest

Choose one of the three data ingest methods based on your monitoring needs.

Event Streaming

EventLogFile

API Queries (SOQL)

Platform Events

Capture [real-time eventsï»¿](https://dt-url.net/fj03qyl) from Salesforce and send them as [Business Events](/docs/observe/business-observability/bo-basic-concepts "Basic concepts of Dynatrace Business Observability.") to Dynatrace.

[RUM ingest](/docs/observe/digital-experience/rum-concepts/rum-overview "Learn about Real User Monitoring, key performance metrics, mobile app monitoring, and more.") should only be used on Managed Environments.

1. Enable **Real-time event Streaming**.
2. In **Setup** > **Event Manager**, enable **Streaming Data** for the events you want to capture.
3. Obtain the credentials needed for the extension to connect to Salesforce.

Enable use cases such as:

* Track Session Hijack, Credential Stuffing and Anomaly Events
* Track important permission sets and changes
* What are the slowest Lightning or Classic pages response times?
* What files are being uploaded, downloaded, and by which user?
* What are the most used reports, who is running them, what queries are being used?
* What are the top API Queries being made?
* What browsers are users using?
* Where is Salesforce being accessed from?
* How many users are using the platform currently, what is the user experience?

The extension uses the [Salesforce Pub/Sub APIï»¿](https://dt-url.net/3843qs9) to list for Event Streaming Events. These events are captured and sent as Business events, or in case of OpenKit as **User Actions**, with all their respective properties sent as **Action Properties**.

For a detailed description of every event and their properties, see the [Salesforce documentationï»¿](https://dt-url.net/g1034yh).

### Currently captured events

| Events | Description |
| --- | --- |
| [ApiAnomalyEventï»¿](https://dt-url.net/ax63qn2) | Track anomalies in how users make API calls. |
| [ApiEventStreamï»¿](https://dt-url.net/1m83q6q) | Track user API queries in your organization. |
| [BulkApiResultEventï»¿](https://dt-url.net/ioc3q4t) | Track when a user downloads the results of a Bulk API request. |
| [ConcurLongRunApexErrEventï»¿](https://dt-url.net/19e3q2i) | Track when a Concurrent Long Running Apex error has occurred. |
| [CredentialStuffingEventï»¿](https://dt-url.net/avg3q25) | Track when a user successfully logs in to Salesforce during an identified credential stuffing attack. |
| [FileEventï»¿](https://dt-url.net/sui3q40) | Track file activity. For example, track when a user downloads or previews a file. |
| [LightningUriEventStreamï»¿](https://dt-url.net/bbk3quc) | Track when a user creates, accesses, updates, or deletes a record in Salesforce Lightning. |
| [ListViewEventStreamï»¿](https://dt-url.net/7a03qz2) | Track when a user accesses data with list views. |
| [LoginAsEventStreamï»¿](https://dt-url.net/o423qb0) | Track when an admin logs into your organization as another user. |
| [LoginEventStreamï»¿](https://dt-url.net/un43qqg) | Track when a user logs in to your organization. |
| [LogoutEventStreamï»¿](https://dt-url.net/7i63qh5) | Track when a user logs out in the Salesforce UI. |
| [PermissionSetEventï»¿](https://dt-url.net/ay83qje) | Track when users are assigned the Modify All Data or View All Data permission through a permission set. |
| [ReportAnomalyEventï»¿](https://dt-url.net/0ga3qkk) | Track anomalies in how users run or export reports. |
| [ReportEventStreamï»¿](https://dt-url.net/9vc3q8c) | Track when a user accesses or exports data with reports. |
| [SessionHijackingEventï»¿](https://dt-url.net/1fe3q8b) | Track when an unauthorized user gains ownership of a Salesforce userâs session with a stolen session identifier. |
| [UriEventStreamï»¿](https://dt-url.net/fig3qk1) | Track when a user creates, accesses, updates, or deletes a record in Salesforce Classic. |

The extension is limited to events that Salesforce produces as **Real-Time Events**.

Choose your reporting mode for Event Streaming data:

### Business Events

For business events, all data is ingested using the business events API.

It can be queried using DQL:

```
fetch bizevents



| filter event.type == "salesforce.ApiEventStream"
```

Visualization of a query result

![img.png](https://dt-cdn.net/images/query-event-streaming-1-1523-1e42763655.png)

Each of the events of type `salesforce.NameOfTheEvent` will have all properties documented by Salesforce.
For instance, see the [properties for an ApiEventStreamï»¿](https://dt-url.net/1m83q6q).

So we can create visualizations using all of these properties.

```
fetch bizevents



| filter event.type == "salesforce.ApiEventStream"



| summarize count(), by: {SourceIp}
```

Visualization of a query result

![img.png](https://dt-cdn.net/images/dql-event-streaming-visualization-1054-0ead02b872.png)

You can get a list of all event types with DQL:

```
fetch bizevents



| filter event.provider == "https://dynatrace--staging.sandbox.my.salesforce.com"



| summarize count(), by: {event.type}
```

Visualization of a query result

![img.png](https://dt-cdn.net/images/dql-event-streaming-event-types-1047-67bd7752e5.png)

#### Example: Get logins by user overtime

```
fetch bizevents



| filter event.type == "salesforce.LoginEventStream"



| makeTimeseries logins=count(), by:{Username}, interval: 5m



| sort logins desc
```

Visualization of a query result

![img.png](https://dt-cdn.net/images/dql-eventstreaming-users-1151-e4c2abe6c6.png)

### OpenKit

To create a custom application to receive the data:

1. In Hub, see **Digital Experience Monitoring** section. Then, select **Generic front end** > **Set up**.
2. Create your custom applicationâname it and choose an icon.
3. Select **Monitor custom application**.

![custom-app-01](https://dt-cdn.net/images/custom-app-01-1251-cec1d8931d.png)

4. In the **Custom application settings**, go to **Instrumentation wizard** and save the `Application ID` for later.

![custom-app-02](https://dt-cdn.net/images/custom-app-02-937-2cdaf63646.png)

5. Now you can enable the extension. For details, go back to the <#enable-extension> section.

The data is sent to the Frontend application that you created, so you can access:

* **Sessions Details**
* The individual properties, by selecting **User Action** > **Perform waterfall analysis**

To use these properties in [User Sessions Query Language](/docs/observe/digital-experience/session-segmentation/custom-queries-segmentation-and-aggregation-of-session-data "Learn how you can access and query user session data based on keywords, syntax, functions, and more."):

1. In the application settings, go to **Session and user action properties**.
2. Create a property.
   Note: the Name must match exactly the property name, see [Salesforce eventsï»¿](https://dt-url.net/bbk3quc).

Example: Capture the rows number

![salesforce-data-04](https://dt-cdn.net/images/data-04-822-d25b8f5d10.png)

Querying the property:

```
SELECT useraction.name, SUM(longProperties.rowsprocessed) FROM useraction WHERE useraction.name STARTSWITH "Report" GROUP BY useraction.name
```

![salesforce-data-05](https://dt-cdn.net/images/data-05-1232-2127c34f50.png)

## EventLogFile

Capture [event log filesï»¿](https://dt-url.net/0a03q0q) from Salesforce and ingest them as [logs](/docs/analyze-explore-automate/logs/lma-log-ingestion/lma-log-ingestion-via-api "Stream log data to Dynatrace using API and have Dynatrace transform it into meaningful log messages.") to Dynatrace.

* EventLogFile needs to be enabled in [Salesforceï»¿](https://dt-url.net/27u3qmr).
* The user must have set permissions to read event log files.

#### Details

1. Create a new EventLogFile configuration by selecting **Configure EventLogFile**.  
   Note that under **Events to capture** all the different Log Files will be disabled by default.
2. Select which log files you would like to ingest.

Do not expect real-time data out of this configuration. Event Log data is [delayed by several hoursï»¿](https://dt-url.net/aqm3qh5) in Salesforce.

#### Visualization

The events are sent as Log Events to Dynatrace, and can be queries using DQL:

```
fetch logs



| filter query.type == "EventLogFile"
```

Visualization of a query result

![img.png](https://dt-cdn.net/images/dql-eventlogfile-1-1058-92cd80e16c.png)

Every property for a certain Event Log File will be available.  
To find fields details, see [EventLogFile Supported Event Typesï»¿](https://dt-url.net/0a03q0q).

#### Example

Get details about `ApexExecution` events:

```
fetch logs



| filter EVENT_TYPE == "ApexExecution"



| fields TIMESTAMP_DERIVED, ENTRY_POINT, EXEC_TIME, CPU_TIME, DB_TOTAL_TIME, NUMBER_SOQL_QUERIES
```

Visualization of a query result

![img.png](https://dt-cdn.net/images/dql-eventlogfile-2-1144-a972a14519.png)

## API Queries (SOQL)

Run [SOQL queriesï»¿](https://dt-url.net/ox23q6n) against Salesforce and ingest the data as [logs](/docs/analyze-explore-automate/logs/lma-log-ingestion/lma-log-ingestion-via-api "Stream log data to Dynatrace using API and have Dynatrace transform it into meaningful log messages.") or Business Events.
The user must have permissions to query the Salesforce API, and read the **Objects** that are being queried.

#### Details

To ingest Salesforce Object Query Language (SOQL) data into Dynatrace,

1. Select **Configure API queries**.
2. Add up to 100 SOQL queries to be executed at the specified interval.
   Each query has the following parameters:

   * **Query Name**âA name for the query that will help you locate this query data later in Dynatrace Logs.
   * **Query**âThe SOQL query to be executed.

     + The query **must** contain at least one datetime field.
     + The placeholder `{last_execution_timestamp}` **must** be used to filter the query results and deduplicate the data.
     + Example: `SELECT Id, CreatedDate, Field, NewValue, OldValue FROM OpportunityFieldHistory WHERE CreatedDate > {last_execution_timestamp}`
   * **Frequency**

     + The frequency can be of type **Interval** or **Cron**.
     + **Interval**âThe query will be every X minutes.
     + **Cron**âThe query will be executed based on the cron expression provided, you can use [crontab guruï»¿](https://dt-url.net/j0o3qxt) to generate the cron expression.

#### Visualization

Let's consider this query configuration:

```
Query Name: Logins



Query: SELECT UserId, COUNT(Id) from LoginHistory WHERE LoginTime > {last_execution_timestamp} GROUP BY UserId
```

![img.png](https://dt-cdn.net/images/query-logins-607-41fe169e79.png)

The results can be obtained with the DQL:

```
fetch logs



| filter query.name == "Logins"
```

Visualization of a query result

![img.png](https://dt-cdn.net/images/dql-api-query-1577-d62b4ba57b.png)

And a chart could be created with:

```
fetch logs



| filter query.name == "Logins"



| makeTimeseries sum(toDouble(expr0)), by: {UserId}, interval: 5m
```

Visualization of a query result

![img.png](https://dt-cdn.net/images/dql-api-query-chart-1139-89bb9c3aa6.png)

You can also fetch all the queries and their text configured for this Salesforce instance:

```
fetch logs



| filter event.provider == "https://dynatrace--staging.sandbox.my.salesforce.com"



| summarize count(), by: {query.name}
```

Visualization of a query result

![img.png](https://dt-cdn.net/images/dql-all-queries-1140-350b2a1991.png)

## Platform Events

Subscribe to custom [Salesforce Platform Eventsï»¿](https://developer.salesforce.com/docs/atlas.en-us.250.0.platform_events.meta/platform_events/platform_events_intro.htm) and ingest them as [Business Events](/docs/observe/business-observability/bo-basic-concepts "Basic concepts of Dynatrace Business Observability.") to Dynatrace.

Platform Events provide a powerful way to send and receive custom event notifications within Salesforce and to external systems. This configuration allows you to capture real-time event data from custom platform events, standard platform events, and change data capture events.

#### Details

To ingest Salesforce Platform Events into Dynatrace:

1. Select **Configure Platform Events**.
2. Add the topics you want to subscribe to. Topics follow these formats:

   * **Custom Platform Events**: `/event/YourCustomEvent__e`
   * **Standard Platform Events**: `/event/LoginEventStream`, `/event/LogoutEventStream`
   * **Change Data Capture**: `/data/ChangeEvents`, `/data/AccountChangeEvent`
3. Configure authentication using one of the supported methods (Connected App, User and Password, or Client ID).

#### Use Cases

Enable use cases such as:

* Monitor custom business processes and workflows
* Track changes to critical Salesforce objects with Change Data Capture
* Integrate with external systems using custom Platform Events
* Build real-time dashboards and alerts based on Salesforce events
* Correlate Salesforce events with other observability data

#### Visualization

Platform Events are sent as Business Events to Dynatrace and can be queried using DQL:

```
fetch bizevents



| filter event.type == "salesforce.YourCustomEvent__e"
```

Example: Query all Platform Events from a Salesforce instance

```
fetch bizevents



| filter event.provider == "https://yourinstance.my.salesforce.com"



| summarize count(), by: {event.type}
```

Each Platform Event will include all custom fields defined on the event, making them available for filtering, grouping, and visualization.

## Extend the retention period for Salesforce Insights data

By default, your ingested data is stored for 30 days. You can adjust the retention time by creating a custom [bucket](/docs/observe/business-observability/bo-event-processing/bo-bucket-assignment "Assign a retention period to business event data in Dynatrace via the classic pipeline.").

To create a custom bucket for a Salesforce event

1. In Dynatrace, go to **Settings** > **Business Observability** > **Bucket assignment**.
2. On the **Business event bucket assignment** page, select **Add rule** and name your rule.
3. In the **Bucket** field, choose your retention period.
4. Add a **Matcher** to your rule by typing or pasting your [matcher-specific DQL query](/docs/observe/business-observability/bo-event-processing/bo-events-processing-matcher "This is the DQL matcher in events in the classic pipeline ."). Events that match your rule will be assigned to your selected bucket. If no rules match, events will be assigned to the default bucket. To assign all your Salesforce events to your bucket, you need to use the matcher containing the `matchesValue` function and your Salesforce URL, as in the example below.

   ```
   matchesValue(event.provider, "https://environment.my.salesforce.com")
   ```
5. Select **Save changes**.

## Troubleshooting

Error logs can be obtained via Dynatrace, by navigating to the Extension page, and selecting **Status** for each monitoring configuration.

Detailed logs can be obtained by creating an [ActiveGate Diagnosis](/docs/ingest-from/dynatrace-activegate/activegate-diagnostics "Learn how to run ActiveGate diagnostics").


---
